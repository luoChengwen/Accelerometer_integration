{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "don't overfit 2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/luoChengwen/Accelerometer_integration/blob/master/don't_overfit_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kWrCiYg3YR2H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from google.colab import files"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pg_OBO2fZS9y",
        "colab_type": "text"
      },
      "source": [
        "###https://www.kaggle.com/c/dont-overfit-ii/data\n",
        "no idea what this data is about ~~~"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kZN0B1FDYVuQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train = files.upload()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vIoLsMPcY5YR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sb"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jSfeKgKcZDTC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train= pd.read_csv('/content/train.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y1psVF3zZKT5",
        "colab_type": "code",
        "outputId": "30b047ca-88e4-4bad-c19b-98861c2eda94",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "train.columns"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['id', 'target', '0', '1', '2', '3', '4', '5', '6', '7',\n",
              "       ...\n",
              "       '290', '291', '292', '293', '294', '295', '296', '297', '298', '299'],\n",
              "      dtype='object', length=302)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cu-Pp767ZLql",
        "colab_type": "code",
        "outputId": "170eeb60-b794-4754-acf5-f5fd2bf8ae93",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        }
      },
      "source": [
        "train.head(4)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>target</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>...</th>\n",
              "      <th>260</th>\n",
              "      <th>261</th>\n",
              "      <th>262</th>\n",
              "      <th>263</th>\n",
              "      <th>264</th>\n",
              "      <th>265</th>\n",
              "      <th>266</th>\n",
              "      <th>267</th>\n",
              "      <th>268</th>\n",
              "      <th>269</th>\n",
              "      <th>270</th>\n",
              "      <th>271</th>\n",
              "      <th>272</th>\n",
              "      <th>273</th>\n",
              "      <th>274</th>\n",
              "      <th>275</th>\n",
              "      <th>276</th>\n",
              "      <th>277</th>\n",
              "      <th>278</th>\n",
              "      <th>279</th>\n",
              "      <th>280</th>\n",
              "      <th>281</th>\n",
              "      <th>282</th>\n",
              "      <th>283</th>\n",
              "      <th>284</th>\n",
              "      <th>285</th>\n",
              "      <th>286</th>\n",
              "      <th>287</th>\n",
              "      <th>288</th>\n",
              "      <th>289</th>\n",
              "      <th>290</th>\n",
              "      <th>291</th>\n",
              "      <th>292</th>\n",
              "      <th>293</th>\n",
              "      <th>294</th>\n",
              "      <th>295</th>\n",
              "      <th>296</th>\n",
              "      <th>297</th>\n",
              "      <th>298</th>\n",
              "      <th>299</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-0.098</td>\n",
              "      <td>2.165</td>\n",
              "      <td>0.681</td>\n",
              "      <td>-0.614</td>\n",
              "      <td>1.309</td>\n",
              "      <td>-0.455</td>\n",
              "      <td>-0.236</td>\n",
              "      <td>0.276</td>\n",
              "      <td>-2.246</td>\n",
              "      <td>1.825</td>\n",
              "      <td>-0.912</td>\n",
              "      <td>-0.107</td>\n",
              "      <td>0.305</td>\n",
              "      <td>0.102</td>\n",
              "      <td>0.826</td>\n",
              "      <td>0.417</td>\n",
              "      <td>0.177</td>\n",
              "      <td>-0.673</td>\n",
              "      <td>-0.503</td>\n",
              "      <td>1.864</td>\n",
              "      <td>0.410</td>\n",
              "      <td>-1.927</td>\n",
              "      <td>0.102</td>\n",
              "      <td>-0.931</td>\n",
              "      <td>1.763</td>\n",
              "      <td>1.449</td>\n",
              "      <td>-1.097</td>\n",
              "      <td>-0.686</td>\n",
              "      <td>-0.250</td>\n",
              "      <td>-1.859</td>\n",
              "      <td>1.125</td>\n",
              "      <td>1.009</td>\n",
              "      <td>-2.296</td>\n",
              "      <td>0.385</td>\n",
              "      <td>-0.876</td>\n",
              "      <td>1.528</td>\n",
              "      <td>-0.144</td>\n",
              "      <td>-1.078</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.681</td>\n",
              "      <td>1.250</td>\n",
              "      <td>-0.565</td>\n",
              "      <td>-1.318</td>\n",
              "      <td>-0.923</td>\n",
              "      <td>0.075</td>\n",
              "      <td>-0.704</td>\n",
              "      <td>2.457</td>\n",
              "      <td>0.771</td>\n",
              "      <td>-0.460</td>\n",
              "      <td>0.569</td>\n",
              "      <td>-1.320</td>\n",
              "      <td>-1.516</td>\n",
              "      <td>-2.145</td>\n",
              "      <td>-1.120</td>\n",
              "      <td>0.156</td>\n",
              "      <td>0.820</td>\n",
              "      <td>-1.049</td>\n",
              "      <td>-1.125</td>\n",
              "      <td>0.484</td>\n",
              "      <td>0.617</td>\n",
              "      <td>1.253</td>\n",
              "      <td>1.248</td>\n",
              "      <td>0.504</td>\n",
              "      <td>-0.802</td>\n",
              "      <td>-0.896</td>\n",
              "      <td>-1.793</td>\n",
              "      <td>-0.284</td>\n",
              "      <td>-0.601</td>\n",
              "      <td>0.569</td>\n",
              "      <td>0.867</td>\n",
              "      <td>1.347</td>\n",
              "      <td>0.504</td>\n",
              "      <td>-0.649</td>\n",
              "      <td>0.672</td>\n",
              "      <td>-2.097</td>\n",
              "      <td>1.051</td>\n",
              "      <td>-0.414</td>\n",
              "      <td>1.038</td>\n",
              "      <td>-1.065</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.081</td>\n",
              "      <td>-0.973</td>\n",
              "      <td>-0.383</td>\n",
              "      <td>0.326</td>\n",
              "      <td>-0.428</td>\n",
              "      <td>0.317</td>\n",
              "      <td>1.172</td>\n",
              "      <td>0.352</td>\n",
              "      <td>0.004</td>\n",
              "      <td>-0.291</td>\n",
              "      <td>2.907</td>\n",
              "      <td>1.085</td>\n",
              "      <td>2.144</td>\n",
              "      <td>1.540</td>\n",
              "      <td>0.584</td>\n",
              "      <td>1.133</td>\n",
              "      <td>1.098</td>\n",
              "      <td>-0.237</td>\n",
              "      <td>-0.498</td>\n",
              "      <td>0.283</td>\n",
              "      <td>-1.100</td>\n",
              "      <td>-0.417</td>\n",
              "      <td>1.382</td>\n",
              "      <td>-0.515</td>\n",
              "      <td>-1.519</td>\n",
              "      <td>0.619</td>\n",
              "      <td>-0.128</td>\n",
              "      <td>0.866</td>\n",
              "      <td>-0.540</td>\n",
              "      <td>1.238</td>\n",
              "      <td>-0.227</td>\n",
              "      <td>0.269</td>\n",
              "      <td>-0.390</td>\n",
              "      <td>-2.721</td>\n",
              "      <td>1.659</td>\n",
              "      <td>0.106</td>\n",
              "      <td>-0.121</td>\n",
              "      <td>1.719</td>\n",
              "      <td>...</td>\n",
              "      <td>0.971</td>\n",
              "      <td>-1.489</td>\n",
              "      <td>0.530</td>\n",
              "      <td>0.917</td>\n",
              "      <td>-0.094</td>\n",
              "      <td>-1.407</td>\n",
              "      <td>0.887</td>\n",
              "      <td>-0.104</td>\n",
              "      <td>-0.583</td>\n",
              "      <td>1.267</td>\n",
              "      <td>-1.667</td>\n",
              "      <td>-2.771</td>\n",
              "      <td>-0.516</td>\n",
              "      <td>1.312</td>\n",
              "      <td>0.491</td>\n",
              "      <td>0.932</td>\n",
              "      <td>2.064</td>\n",
              "      <td>0.422</td>\n",
              "      <td>1.215</td>\n",
              "      <td>2.012</td>\n",
              "      <td>0.043</td>\n",
              "      <td>-0.307</td>\n",
              "      <td>-0.059</td>\n",
              "      <td>1.121</td>\n",
              "      <td>1.333</td>\n",
              "      <td>0.211</td>\n",
              "      <td>1.753</td>\n",
              "      <td>0.053</td>\n",
              "      <td>1.274</td>\n",
              "      <td>-0.612</td>\n",
              "      <td>-0.165</td>\n",
              "      <td>-1.695</td>\n",
              "      <td>-1.257</td>\n",
              "      <td>1.359</td>\n",
              "      <td>-0.808</td>\n",
              "      <td>-1.624</td>\n",
              "      <td>-0.458</td>\n",
              "      <td>-1.099</td>\n",
              "      <td>-0.936</td>\n",
              "      <td>0.973</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-0.523</td>\n",
              "      <td>-0.089</td>\n",
              "      <td>-0.348</td>\n",
              "      <td>0.148</td>\n",
              "      <td>-0.022</td>\n",
              "      <td>0.404</td>\n",
              "      <td>-0.023</td>\n",
              "      <td>-0.172</td>\n",
              "      <td>0.137</td>\n",
              "      <td>0.183</td>\n",
              "      <td>0.459</td>\n",
              "      <td>0.478</td>\n",
              "      <td>-0.425</td>\n",
              "      <td>0.352</td>\n",
              "      <td>1.095</td>\n",
              "      <td>0.300</td>\n",
              "      <td>-1.044</td>\n",
              "      <td>0.270</td>\n",
              "      <td>-1.038</td>\n",
              "      <td>0.144</td>\n",
              "      <td>-1.658</td>\n",
              "      <td>-0.946</td>\n",
              "      <td>0.633</td>\n",
              "      <td>-0.772</td>\n",
              "      <td>1.786</td>\n",
              "      <td>0.136</td>\n",
              "      <td>-0.103</td>\n",
              "      <td>-1.223</td>\n",
              "      <td>2.273</td>\n",
              "      <td>0.055</td>\n",
              "      <td>-2.032</td>\n",
              "      <td>-0.452</td>\n",
              "      <td>0.064</td>\n",
              "      <td>0.924</td>\n",
              "      <td>-0.692</td>\n",
              "      <td>-0.067</td>\n",
              "      <td>-0.917</td>\n",
              "      <td>1.896</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.540</td>\n",
              "      <td>-0.299</td>\n",
              "      <td>1.074</td>\n",
              "      <td>-0.748</td>\n",
              "      <td>1.086</td>\n",
              "      <td>-0.766</td>\n",
              "      <td>-0.931</td>\n",
              "      <td>0.432</td>\n",
              "      <td>1.345</td>\n",
              "      <td>-0.491</td>\n",
              "      <td>-1.602</td>\n",
              "      <td>-0.727</td>\n",
              "      <td>0.346</td>\n",
              "      <td>0.780</td>\n",
              "      <td>-0.527</td>\n",
              "      <td>-1.122</td>\n",
              "      <td>-0.208</td>\n",
              "      <td>-0.730</td>\n",
              "      <td>-0.302</td>\n",
              "      <td>2.535</td>\n",
              "      <td>-1.045</td>\n",
              "      <td>0.037</td>\n",
              "      <td>0.020</td>\n",
              "      <td>1.373</td>\n",
              "      <td>0.456</td>\n",
              "      <td>-0.277</td>\n",
              "      <td>1.381</td>\n",
              "      <td>1.843</td>\n",
              "      <td>0.749</td>\n",
              "      <td>0.202</td>\n",
              "      <td>0.013</td>\n",
              "      <td>0.263</td>\n",
              "      <td>-1.222</td>\n",
              "      <td>0.726</td>\n",
              "      <td>1.444</td>\n",
              "      <td>-1.165</td>\n",
              "      <td>-1.544</td>\n",
              "      <td>0.004</td>\n",
              "      <td>0.800</td>\n",
              "      <td>-1.211</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.067</td>\n",
              "      <td>-0.021</td>\n",
              "      <td>0.392</td>\n",
              "      <td>-1.637</td>\n",
              "      <td>-0.446</td>\n",
              "      <td>-0.725</td>\n",
              "      <td>-1.035</td>\n",
              "      <td>0.834</td>\n",
              "      <td>0.503</td>\n",
              "      <td>0.274</td>\n",
              "      <td>0.335</td>\n",
              "      <td>-1.148</td>\n",
              "      <td>0.067</td>\n",
              "      <td>-1.010</td>\n",
              "      <td>1.048</td>\n",
              "      <td>-1.442</td>\n",
              "      <td>0.210</td>\n",
              "      <td>0.836</td>\n",
              "      <td>-0.326</td>\n",
              "      <td>0.716</td>\n",
              "      <td>-0.764</td>\n",
              "      <td>0.248</td>\n",
              "      <td>-1.308</td>\n",
              "      <td>2.127</td>\n",
              "      <td>0.365</td>\n",
              "      <td>0.296</td>\n",
              "      <td>-0.808</td>\n",
              "      <td>1.854</td>\n",
              "      <td>0.118</td>\n",
              "      <td>0.380</td>\n",
              "      <td>0.999</td>\n",
              "      <td>-1.171</td>\n",
              "      <td>2.798</td>\n",
              "      <td>0.394</td>\n",
              "      <td>-1.048</td>\n",
              "      <td>1.078</td>\n",
              "      <td>0.401</td>\n",
              "      <td>-0.486</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.083</td>\n",
              "      <td>-0.831</td>\n",
              "      <td>1.251</td>\n",
              "      <td>-0.206</td>\n",
              "      <td>-0.933</td>\n",
              "      <td>-1.215</td>\n",
              "      <td>0.281</td>\n",
              "      <td>0.512</td>\n",
              "      <td>-0.424</td>\n",
              "      <td>0.769</td>\n",
              "      <td>0.223</td>\n",
              "      <td>-0.710</td>\n",
              "      <td>2.725</td>\n",
              "      <td>0.176</td>\n",
              "      <td>0.845</td>\n",
              "      <td>-1.226</td>\n",
              "      <td>1.527</td>\n",
              "      <td>-1.701</td>\n",
              "      <td>0.597</td>\n",
              "      <td>0.150</td>\n",
              "      <td>1.864</td>\n",
              "      <td>0.322</td>\n",
              "      <td>-0.214</td>\n",
              "      <td>1.282</td>\n",
              "      <td>0.408</td>\n",
              "      <td>-0.910</td>\n",
              "      <td>1.020</td>\n",
              "      <td>-0.299</td>\n",
              "      <td>-1.574</td>\n",
              "      <td>-1.618</td>\n",
              "      <td>-0.404</td>\n",
              "      <td>0.640</td>\n",
              "      <td>-0.595</td>\n",
              "      <td>-0.966</td>\n",
              "      <td>0.900</td>\n",
              "      <td>0.467</td>\n",
              "      <td>-0.562</td>\n",
              "      <td>-0.254</td>\n",
              "      <td>-0.533</td>\n",
              "      <td>0.238</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>4 rows × 302 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   id  target      0      1      2  ...    295    296    297    298    299\n",
              "0   0     1.0 -0.098  2.165  0.681  ... -2.097  1.051 -0.414  1.038 -1.065\n",
              "1   1     0.0  1.081 -0.973 -0.383  ... -1.624 -0.458 -1.099 -0.936  0.973\n",
              "2   2     1.0 -0.523 -0.089 -0.348  ... -1.165 -1.544  0.004  0.800 -1.211\n",
              "3   3     1.0  0.067 -0.021  0.392  ...  0.467 -0.562 -0.254 -0.533  0.238\n",
              "\n",
              "[4 rows x 302 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q2C3ikazZOIA",
        "colab_type": "code",
        "outputId": "dca2751d-ceff-4408-fff3-614bdfa625af",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "print(sum(train.isnull().sum()))\n",
        "print(np.shape(train))\n",
        "print(np.unique(train.target))\n",
        "train = train.drop(columns=['id'])\n",
        "a = train.corr()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "(250, 302)\n",
            "[0. 1.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mTPk6FzldZwg",
        "colab_type": "code",
        "outputId": "15777d62-df43-4465-e675-5f06f7d90cda",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        }
      },
      "source": [
        "type(a)\n",
        "a.head(3)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>target</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>...</th>\n",
              "      <th>260</th>\n",
              "      <th>261</th>\n",
              "      <th>262</th>\n",
              "      <th>263</th>\n",
              "      <th>264</th>\n",
              "      <th>265</th>\n",
              "      <th>266</th>\n",
              "      <th>267</th>\n",
              "      <th>268</th>\n",
              "      <th>269</th>\n",
              "      <th>270</th>\n",
              "      <th>271</th>\n",
              "      <th>272</th>\n",
              "      <th>273</th>\n",
              "      <th>274</th>\n",
              "      <th>275</th>\n",
              "      <th>276</th>\n",
              "      <th>277</th>\n",
              "      <th>278</th>\n",
              "      <th>279</th>\n",
              "      <th>280</th>\n",
              "      <th>281</th>\n",
              "      <th>282</th>\n",
              "      <th>283</th>\n",
              "      <th>284</th>\n",
              "      <th>285</th>\n",
              "      <th>286</th>\n",
              "      <th>287</th>\n",
              "      <th>288</th>\n",
              "      <th>289</th>\n",
              "      <th>290</th>\n",
              "      <th>291</th>\n",
              "      <th>292</th>\n",
              "      <th>293</th>\n",
              "      <th>294</th>\n",
              "      <th>295</th>\n",
              "      <th>296</th>\n",
              "      <th>297</th>\n",
              "      <th>298</th>\n",
              "      <th>299</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>target</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.108966</td>\n",
              "      <td>-0.073319</td>\n",
              "      <td>-0.015141</td>\n",
              "      <td>0.011549</td>\n",
              "      <td>-0.114726</td>\n",
              "      <td>-0.050329</td>\n",
              "      <td>-0.057063</td>\n",
              "      <td>0.004239</td>\n",
              "      <td>-0.048443</td>\n",
              "      <td>-0.085947</td>\n",
              "      <td>-0.013612</td>\n",
              "      <td>-0.031463</td>\n",
              "      <td>0.009103</td>\n",
              "      <td>0.107828</td>\n",
              "      <td>0.063624</td>\n",
              "      <td>-0.094138</td>\n",
              "      <td>-0.144267</td>\n",
              "      <td>0.110998</td>\n",
              "      <td>0.050365</td>\n",
              "      <td>-0.048693</td>\n",
              "      <td>-0.002429</td>\n",
              "      <td>0.045513</td>\n",
              "      <td>-0.018173</td>\n",
              "      <td>-0.051417</td>\n",
              "      <td>0.173096</td>\n",
              "      <td>0.072617</td>\n",
              "      <td>-0.097438</td>\n",
              "      <td>0.056375</td>\n",
              "      <td>-0.041337</td>\n",
              "      <td>0.055357</td>\n",
              "      <td>0.132705</td>\n",
              "      <td>-0.018271</td>\n",
              "      <td>-0.031088</td>\n",
              "      <td>0.373608</td>\n",
              "      <td>-0.032472</td>\n",
              "      <td>-0.015633</td>\n",
              "      <td>0.060833</td>\n",
              "      <td>-0.007289</td>\n",
              "      <td>-0.040545</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.046107</td>\n",
              "      <td>-0.057885</td>\n",
              "      <td>-0.077726</td>\n",
              "      <td>0.018638</td>\n",
              "      <td>0.036513</td>\n",
              "      <td>0.008513</td>\n",
              "      <td>0.017778</td>\n",
              "      <td>0.069989</td>\n",
              "      <td>-0.082616</td>\n",
              "      <td>-0.009661</td>\n",
              "      <td>-0.035870</td>\n",
              "      <td>0.016826</td>\n",
              "      <td>0.113909</td>\n",
              "      <td>0.003967</td>\n",
              "      <td>0.008408</td>\n",
              "      <td>-0.070916</td>\n",
              "      <td>-0.117467</td>\n",
              "      <td>-0.033894</td>\n",
              "      <td>0.014398</td>\n",
              "      <td>-0.071330</td>\n",
              "      <td>0.078811</td>\n",
              "      <td>-0.088500</td>\n",
              "      <td>0.044530</td>\n",
              "      <td>0.078050</td>\n",
              "      <td>-0.033451</td>\n",
              "      <td>0.087762</td>\n",
              "      <td>-0.045833</td>\n",
              "      <td>-0.066705</td>\n",
              "      <td>-0.072923</td>\n",
              "      <td>0.127213</td>\n",
              "      <td>0.039675</td>\n",
              "      <td>0.055694</td>\n",
              "      <td>-0.088930</td>\n",
              "      <td>-0.034363</td>\n",
              "      <td>-0.031964</td>\n",
              "      <td>-0.170501</td>\n",
              "      <td>0.007434</td>\n",
              "      <td>0.056810</td>\n",
              "      <td>-0.134760</td>\n",
              "      <td>-0.075475</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.108966</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.003871</td>\n",
              "      <td>-0.010487</td>\n",
              "      <td>-0.047819</td>\n",
              "      <td>0.013967</td>\n",
              "      <td>0.070091</td>\n",
              "      <td>-0.022537</td>\n",
              "      <td>0.002832</td>\n",
              "      <td>-0.060031</td>\n",
              "      <td>-0.052356</td>\n",
              "      <td>0.059263</td>\n",
              "      <td>-0.014094</td>\n",
              "      <td>-0.060538</td>\n",
              "      <td>0.049368</td>\n",
              "      <td>-0.017255</td>\n",
              "      <td>-0.047828</td>\n",
              "      <td>-0.031047</td>\n",
              "      <td>0.013264</td>\n",
              "      <td>-0.043416</td>\n",
              "      <td>-0.005945</td>\n",
              "      <td>-0.000825</td>\n",
              "      <td>0.030775</td>\n",
              "      <td>-0.031150</td>\n",
              "      <td>-0.055822</td>\n",
              "      <td>-0.030686</td>\n",
              "      <td>-0.019718</td>\n",
              "      <td>-0.005644</td>\n",
              "      <td>0.027852</td>\n",
              "      <td>0.035622</td>\n",
              "      <td>-0.008151</td>\n",
              "      <td>0.079281</td>\n",
              "      <td>0.062204</td>\n",
              "      <td>-0.084648</td>\n",
              "      <td>0.077961</td>\n",
              "      <td>-0.044631</td>\n",
              "      <td>0.023800</td>\n",
              "      <td>0.124808</td>\n",
              "      <td>-0.003085</td>\n",
              "      <td>-0.044664</td>\n",
              "      <td>...</td>\n",
              "      <td>0.015049</td>\n",
              "      <td>0.101032</td>\n",
              "      <td>0.009942</td>\n",
              "      <td>0.008299</td>\n",
              "      <td>-0.018995</td>\n",
              "      <td>-0.019378</td>\n",
              "      <td>0.120621</td>\n",
              "      <td>-0.027320</td>\n",
              "      <td>-0.041115</td>\n",
              "      <td>0.036252</td>\n",
              "      <td>-0.013818</td>\n",
              "      <td>-0.022710</td>\n",
              "      <td>0.007584</td>\n",
              "      <td>-0.020473</td>\n",
              "      <td>0.075098</td>\n",
              "      <td>0.054375</td>\n",
              "      <td>-0.028977</td>\n",
              "      <td>-0.041199</td>\n",
              "      <td>0.022243</td>\n",
              "      <td>-0.102684</td>\n",
              "      <td>-0.024006</td>\n",
              "      <td>0.037312</td>\n",
              "      <td>0.135009</td>\n",
              "      <td>0.029303</td>\n",
              "      <td>0.031001</td>\n",
              "      <td>0.033796</td>\n",
              "      <td>0.164564</td>\n",
              "      <td>0.012922</td>\n",
              "      <td>-0.027903</td>\n",
              "      <td>0.034457</td>\n",
              "      <td>-0.023230</td>\n",
              "      <td>0.053416</td>\n",
              "      <td>-0.143668</td>\n",
              "      <td>-0.007530</td>\n",
              "      <td>-0.060824</td>\n",
              "      <td>-0.024839</td>\n",
              "      <td>-0.051288</td>\n",
              "      <td>0.029143</td>\n",
              "      <td>0.065951</td>\n",
              "      <td>0.038523</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.073319</td>\n",
              "      <td>-0.003871</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.013573</td>\n",
              "      <td>-0.018884</td>\n",
              "      <td>0.086743</td>\n",
              "      <td>-0.028023</td>\n",
              "      <td>-0.032914</td>\n",
              "      <td>-0.066416</td>\n",
              "      <td>-0.027201</td>\n",
              "      <td>0.035512</td>\n",
              "      <td>0.020358</td>\n",
              "      <td>0.003761</td>\n",
              "      <td>0.027094</td>\n",
              "      <td>-0.111360</td>\n",
              "      <td>-0.030637</td>\n",
              "      <td>-0.084130</td>\n",
              "      <td>0.023147</td>\n",
              "      <td>-0.115217</td>\n",
              "      <td>0.065643</td>\n",
              "      <td>0.051571</td>\n",
              "      <td>0.055748</td>\n",
              "      <td>-0.084465</td>\n",
              "      <td>0.039373</td>\n",
              "      <td>-0.033250</td>\n",
              "      <td>0.069004</td>\n",
              "      <td>0.105601</td>\n",
              "      <td>0.011806</td>\n",
              "      <td>-0.031546</td>\n",
              "      <td>-0.134291</td>\n",
              "      <td>-0.058598</td>\n",
              "      <td>0.022960</td>\n",
              "      <td>0.029663</td>\n",
              "      <td>-0.006578</td>\n",
              "      <td>-0.029111</td>\n",
              "      <td>-0.003974</td>\n",
              "      <td>0.032745</td>\n",
              "      <td>0.128600</td>\n",
              "      <td>0.088257</td>\n",
              "      <td>0.014338</td>\n",
              "      <td>...</td>\n",
              "      <td>0.005799</td>\n",
              "      <td>0.029290</td>\n",
              "      <td>-0.055173</td>\n",
              "      <td>-0.083558</td>\n",
              "      <td>0.064371</td>\n",
              "      <td>-0.025943</td>\n",
              "      <td>-0.145135</td>\n",
              "      <td>0.032671</td>\n",
              "      <td>0.062708</td>\n",
              "      <td>0.030416</td>\n",
              "      <td>0.026795</td>\n",
              "      <td>-0.020315</td>\n",
              "      <td>-0.006171</td>\n",
              "      <td>-0.146324</td>\n",
              "      <td>-0.015831</td>\n",
              "      <td>-0.049975</td>\n",
              "      <td>-0.031120</td>\n",
              "      <td>0.070214</td>\n",
              "      <td>-0.016917</td>\n",
              "      <td>0.032828</td>\n",
              "      <td>0.098995</td>\n",
              "      <td>0.119581</td>\n",
              "      <td>-0.096928</td>\n",
              "      <td>0.066795</td>\n",
              "      <td>-0.047459</td>\n",
              "      <td>0.058864</td>\n",
              "      <td>0.041791</td>\n",
              "      <td>0.096635</td>\n",
              "      <td>-0.131251</td>\n",
              "      <td>-0.070546</td>\n",
              "      <td>-0.006300</td>\n",
              "      <td>-0.077365</td>\n",
              "      <td>-0.021583</td>\n",
              "      <td>-0.054171</td>\n",
              "      <td>-0.046174</td>\n",
              "      <td>0.042820</td>\n",
              "      <td>-0.127499</td>\n",
              "      <td>0.065883</td>\n",
              "      <td>0.055470</td>\n",
              "      <td>-0.056612</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3 rows × 301 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          target         0         1  ...       297       298       299\n",
              "target  1.000000  0.108966 -0.073319  ...  0.056810 -0.134760 -0.075475\n",
              "0       0.108966  1.000000 -0.003871  ...  0.029143  0.065951  0.038523\n",
              "1      -0.073319 -0.003871  1.000000  ...  0.065883  0.055470 -0.056612\n",
              "\n",
              "[3 rows x 301 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y6Sf1NDWdeew",
        "colab_type": "code",
        "outputId": "c652ea1c-4618-4ebb-cb1c-90f89ba53d20",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        }
      },
      "source": [
        "a=abs(a)\n",
        "a.sort_values(by= ['target'],inplace=True)\n",
        "a.head(10)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>target</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>...</th>\n",
              "      <th>260</th>\n",
              "      <th>261</th>\n",
              "      <th>262</th>\n",
              "      <th>263</th>\n",
              "      <th>264</th>\n",
              "      <th>265</th>\n",
              "      <th>266</th>\n",
              "      <th>267</th>\n",
              "      <th>268</th>\n",
              "      <th>269</th>\n",
              "      <th>270</th>\n",
              "      <th>271</th>\n",
              "      <th>272</th>\n",
              "      <th>273</th>\n",
              "      <th>274</th>\n",
              "      <th>275</th>\n",
              "      <th>276</th>\n",
              "      <th>277</th>\n",
              "      <th>278</th>\n",
              "      <th>279</th>\n",
              "      <th>280</th>\n",
              "      <th>281</th>\n",
              "      <th>282</th>\n",
              "      <th>283</th>\n",
              "      <th>284</th>\n",
              "      <th>285</th>\n",
              "      <th>286</th>\n",
              "      <th>287</th>\n",
              "      <th>288</th>\n",
              "      <th>289</th>\n",
              "      <th>290</th>\n",
              "      <th>291</th>\n",
              "      <th>292</th>\n",
              "      <th>293</th>\n",
              "      <th>294</th>\n",
              "      <th>295</th>\n",
              "      <th>296</th>\n",
              "      <th>297</th>\n",
              "      <th>298</th>\n",
              "      <th>299</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>75</th>\n",
              "      <td>0.000694</td>\n",
              "      <td>0.023369</td>\n",
              "      <td>0.117148</td>\n",
              "      <td>0.002817</td>\n",
              "      <td>0.121211</td>\n",
              "      <td>0.016941</td>\n",
              "      <td>0.044401</td>\n",
              "      <td>0.178214</td>\n",
              "      <td>0.067901</td>\n",
              "      <td>0.093527</td>\n",
              "      <td>0.038796</td>\n",
              "      <td>0.126527</td>\n",
              "      <td>0.019367</td>\n",
              "      <td>0.032059</td>\n",
              "      <td>0.034279</td>\n",
              "      <td>0.038720</td>\n",
              "      <td>0.157005</td>\n",
              "      <td>0.026081</td>\n",
              "      <td>0.047516</td>\n",
              "      <td>0.116459</td>\n",
              "      <td>0.147142</td>\n",
              "      <td>0.031621</td>\n",
              "      <td>0.090825</td>\n",
              "      <td>0.020440</td>\n",
              "      <td>0.070300</td>\n",
              "      <td>0.032529</td>\n",
              "      <td>0.043072</td>\n",
              "      <td>0.051058</td>\n",
              "      <td>0.035228</td>\n",
              "      <td>0.017756</td>\n",
              "      <td>0.086269</td>\n",
              "      <td>0.045879</td>\n",
              "      <td>0.041142</td>\n",
              "      <td>0.259315</td>\n",
              "      <td>0.006638</td>\n",
              "      <td>0.031297</td>\n",
              "      <td>0.127569</td>\n",
              "      <td>0.040404</td>\n",
              "      <td>0.049786</td>\n",
              "      <td>0.040331</td>\n",
              "      <td>...</td>\n",
              "      <td>0.013533</td>\n",
              "      <td>0.015714</td>\n",
              "      <td>0.108636</td>\n",
              "      <td>0.002225</td>\n",
              "      <td>0.063496</td>\n",
              "      <td>0.111903</td>\n",
              "      <td>0.171761</td>\n",
              "      <td>0.177139</td>\n",
              "      <td>0.089485</td>\n",
              "      <td>0.142021</td>\n",
              "      <td>0.031115</td>\n",
              "      <td>0.011381</td>\n",
              "      <td>0.048668</td>\n",
              "      <td>0.114370</td>\n",
              "      <td>0.064290</td>\n",
              "      <td>0.028541</td>\n",
              "      <td>0.023088</td>\n",
              "      <td>0.016414</td>\n",
              "      <td>0.055959</td>\n",
              "      <td>0.010750</td>\n",
              "      <td>0.020152</td>\n",
              "      <td>0.054174</td>\n",
              "      <td>0.047220</td>\n",
              "      <td>0.019964</td>\n",
              "      <td>0.098302</td>\n",
              "      <td>0.151724</td>\n",
              "      <td>0.113582</td>\n",
              "      <td>0.029680</td>\n",
              "      <td>0.054377</td>\n",
              "      <td>0.028299</td>\n",
              "      <td>0.069751</td>\n",
              "      <td>0.005664</td>\n",
              "      <td>0.001684</td>\n",
              "      <td>0.078773</td>\n",
              "      <td>0.039451</td>\n",
              "      <td>0.032520</td>\n",
              "      <td>0.034023</td>\n",
              "      <td>0.053945</td>\n",
              "      <td>0.049781</td>\n",
              "      <td>0.000569</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>195</th>\n",
              "      <td>0.000909</td>\n",
              "      <td>0.101548</td>\n",
              "      <td>0.156044</td>\n",
              "      <td>0.099843</td>\n",
              "      <td>0.001879</td>\n",
              "      <td>0.127113</td>\n",
              "      <td>0.046254</td>\n",
              "      <td>0.025200</td>\n",
              "      <td>0.011123</td>\n",
              "      <td>0.053055</td>\n",
              "      <td>0.163375</td>\n",
              "      <td>0.030072</td>\n",
              "      <td>0.021025</td>\n",
              "      <td>0.041526</td>\n",
              "      <td>0.079727</td>\n",
              "      <td>0.001540</td>\n",
              "      <td>0.041791</td>\n",
              "      <td>0.013536</td>\n",
              "      <td>0.070691</td>\n",
              "      <td>0.013752</td>\n",
              "      <td>0.056762</td>\n",
              "      <td>0.004144</td>\n",
              "      <td>0.062373</td>\n",
              "      <td>0.020544</td>\n",
              "      <td>0.005675</td>\n",
              "      <td>0.065381</td>\n",
              "      <td>0.112287</td>\n",
              "      <td>0.086019</td>\n",
              "      <td>0.026499</td>\n",
              "      <td>0.051109</td>\n",
              "      <td>0.033305</td>\n",
              "      <td>0.019044</td>\n",
              "      <td>0.112719</td>\n",
              "      <td>0.033802</td>\n",
              "      <td>0.059315</td>\n",
              "      <td>0.000020</td>\n",
              "      <td>0.071111</td>\n",
              "      <td>0.102875</td>\n",
              "      <td>0.073745</td>\n",
              "      <td>0.039519</td>\n",
              "      <td>...</td>\n",
              "      <td>0.041646</td>\n",
              "      <td>0.024452</td>\n",
              "      <td>0.100375</td>\n",
              "      <td>0.039725</td>\n",
              "      <td>0.096258</td>\n",
              "      <td>0.082970</td>\n",
              "      <td>0.020718</td>\n",
              "      <td>0.000294</td>\n",
              "      <td>0.006912</td>\n",
              "      <td>0.093290</td>\n",
              "      <td>0.041458</td>\n",
              "      <td>0.043712</td>\n",
              "      <td>0.033371</td>\n",
              "      <td>0.088812</td>\n",
              "      <td>0.077651</td>\n",
              "      <td>0.058331</td>\n",
              "      <td>0.035611</td>\n",
              "      <td>0.023143</td>\n",
              "      <td>0.016981</td>\n",
              "      <td>0.059143</td>\n",
              "      <td>0.083477</td>\n",
              "      <td>0.086354</td>\n",
              "      <td>0.022174</td>\n",
              "      <td>0.046554</td>\n",
              "      <td>0.023070</td>\n",
              "      <td>0.099118</td>\n",
              "      <td>0.006956</td>\n",
              "      <td>0.063699</td>\n",
              "      <td>0.042966</td>\n",
              "      <td>0.004951</td>\n",
              "      <td>0.044493</td>\n",
              "      <td>0.054086</td>\n",
              "      <td>0.006081</td>\n",
              "      <td>0.024001</td>\n",
              "      <td>0.046961</td>\n",
              "      <td>0.154013</td>\n",
              "      <td>0.012301</td>\n",
              "      <td>0.094601</td>\n",
              "      <td>0.009346</td>\n",
              "      <td>0.074856</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>172</th>\n",
              "      <td>0.001375</td>\n",
              "      <td>0.058438</td>\n",
              "      <td>0.035819</td>\n",
              "      <td>0.007705</td>\n",
              "      <td>0.094334</td>\n",
              "      <td>0.037648</td>\n",
              "      <td>0.013376</td>\n",
              "      <td>0.091900</td>\n",
              "      <td>0.008001</td>\n",
              "      <td>0.087341</td>\n",
              "      <td>0.031627</td>\n",
              "      <td>0.045359</td>\n",
              "      <td>0.122039</td>\n",
              "      <td>0.019432</td>\n",
              "      <td>0.002200</td>\n",
              "      <td>0.041614</td>\n",
              "      <td>0.099545</td>\n",
              "      <td>0.001003</td>\n",
              "      <td>0.029369</td>\n",
              "      <td>0.048492</td>\n",
              "      <td>0.057609</td>\n",
              "      <td>0.091590</td>\n",
              "      <td>0.013165</td>\n",
              "      <td>0.008939</td>\n",
              "      <td>0.007967</td>\n",
              "      <td>0.053717</td>\n",
              "      <td>0.047756</td>\n",
              "      <td>0.037380</td>\n",
              "      <td>0.079617</td>\n",
              "      <td>0.003976</td>\n",
              "      <td>0.030586</td>\n",
              "      <td>0.050395</td>\n",
              "      <td>0.101914</td>\n",
              "      <td>0.086756</td>\n",
              "      <td>0.085835</td>\n",
              "      <td>0.038243</td>\n",
              "      <td>0.076499</td>\n",
              "      <td>0.114480</td>\n",
              "      <td>0.079669</td>\n",
              "      <td>0.028728</td>\n",
              "      <td>...</td>\n",
              "      <td>0.015435</td>\n",
              "      <td>0.046168</td>\n",
              "      <td>0.066347</td>\n",
              "      <td>0.000917</td>\n",
              "      <td>0.037487</td>\n",
              "      <td>0.070016</td>\n",
              "      <td>0.005511</td>\n",
              "      <td>0.021457</td>\n",
              "      <td>0.025069</td>\n",
              "      <td>0.113896</td>\n",
              "      <td>0.060023</td>\n",
              "      <td>0.088481</td>\n",
              "      <td>0.116186</td>\n",
              "      <td>0.060343</td>\n",
              "      <td>0.060904</td>\n",
              "      <td>0.075196</td>\n",
              "      <td>0.019987</td>\n",
              "      <td>0.062899</td>\n",
              "      <td>0.048468</td>\n",
              "      <td>0.027940</td>\n",
              "      <td>0.001640</td>\n",
              "      <td>0.041164</td>\n",
              "      <td>0.013733</td>\n",
              "      <td>0.092814</td>\n",
              "      <td>0.027589</td>\n",
              "      <td>0.038402</td>\n",
              "      <td>0.003779</td>\n",
              "      <td>0.079384</td>\n",
              "      <td>0.091730</td>\n",
              "      <td>0.024045</td>\n",
              "      <td>0.056747</td>\n",
              "      <td>0.096137</td>\n",
              "      <td>0.069239</td>\n",
              "      <td>0.028020</td>\n",
              "      <td>0.021364</td>\n",
              "      <td>0.037362</td>\n",
              "      <td>0.001565</td>\n",
              "      <td>0.028213</td>\n",
              "      <td>0.011222</td>\n",
              "      <td>0.076948</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>224</th>\n",
              "      <td>0.002248</td>\n",
              "      <td>0.024380</td>\n",
              "      <td>0.067241</td>\n",
              "      <td>0.015143</td>\n",
              "      <td>0.170201</td>\n",
              "      <td>0.086168</td>\n",
              "      <td>0.059768</td>\n",
              "      <td>0.112911</td>\n",
              "      <td>0.005257</td>\n",
              "      <td>0.068000</td>\n",
              "      <td>0.011421</td>\n",
              "      <td>0.103233</td>\n",
              "      <td>0.013096</td>\n",
              "      <td>0.053004</td>\n",
              "      <td>0.055353</td>\n",
              "      <td>0.066795</td>\n",
              "      <td>0.036150</td>\n",
              "      <td>0.014013</td>\n",
              "      <td>0.043497</td>\n",
              "      <td>0.045130</td>\n",
              "      <td>0.026359</td>\n",
              "      <td>0.008793</td>\n",
              "      <td>0.035284</td>\n",
              "      <td>0.035795</td>\n",
              "      <td>0.013139</td>\n",
              "      <td>0.008264</td>\n",
              "      <td>0.046219</td>\n",
              "      <td>0.016083</td>\n",
              "      <td>0.069432</td>\n",
              "      <td>0.054589</td>\n",
              "      <td>0.044312</td>\n",
              "      <td>0.016777</td>\n",
              "      <td>0.073360</td>\n",
              "      <td>0.108177</td>\n",
              "      <td>0.000425</td>\n",
              "      <td>0.051909</td>\n",
              "      <td>0.028808</td>\n",
              "      <td>0.010937</td>\n",
              "      <td>0.022997</td>\n",
              "      <td>0.058787</td>\n",
              "      <td>...</td>\n",
              "      <td>0.019452</td>\n",
              "      <td>0.046933</td>\n",
              "      <td>0.097792</td>\n",
              "      <td>0.062947</td>\n",
              "      <td>0.045215</td>\n",
              "      <td>0.033858</td>\n",
              "      <td>0.055859</td>\n",
              "      <td>0.044832</td>\n",
              "      <td>0.073408</td>\n",
              "      <td>0.069066</td>\n",
              "      <td>0.037903</td>\n",
              "      <td>0.000560</td>\n",
              "      <td>0.007242</td>\n",
              "      <td>0.050070</td>\n",
              "      <td>0.020115</td>\n",
              "      <td>0.030707</td>\n",
              "      <td>0.109212</td>\n",
              "      <td>0.055412</td>\n",
              "      <td>0.081648</td>\n",
              "      <td>0.023033</td>\n",
              "      <td>0.019788</td>\n",
              "      <td>0.090387</td>\n",
              "      <td>0.009515</td>\n",
              "      <td>0.038546</td>\n",
              "      <td>0.026798</td>\n",
              "      <td>0.004973</td>\n",
              "      <td>0.019255</td>\n",
              "      <td>0.031303</td>\n",
              "      <td>0.093514</td>\n",
              "      <td>0.003243</td>\n",
              "      <td>0.009755</td>\n",
              "      <td>0.015072</td>\n",
              "      <td>0.003834</td>\n",
              "      <td>0.011203</td>\n",
              "      <td>0.105588</td>\n",
              "      <td>0.052905</td>\n",
              "      <td>0.079265</td>\n",
              "      <td>0.010470</td>\n",
              "      <td>0.138338</td>\n",
              "      <td>0.087211</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>0.002429</td>\n",
              "      <td>0.000825</td>\n",
              "      <td>0.055748</td>\n",
              "      <td>0.007613</td>\n",
              "      <td>0.001301</td>\n",
              "      <td>0.052647</td>\n",
              "      <td>0.037632</td>\n",
              "      <td>0.090969</td>\n",
              "      <td>0.037427</td>\n",
              "      <td>0.060797</td>\n",
              "      <td>0.066107</td>\n",
              "      <td>0.046507</td>\n",
              "      <td>0.009396</td>\n",
              "      <td>0.035017</td>\n",
              "      <td>0.045642</td>\n",
              "      <td>0.003312</td>\n",
              "      <td>0.018586</td>\n",
              "      <td>0.062880</td>\n",
              "      <td>0.005705</td>\n",
              "      <td>0.035947</td>\n",
              "      <td>0.050480</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.002561</td>\n",
              "      <td>0.070983</td>\n",
              "      <td>0.030310</td>\n",
              "      <td>0.008438</td>\n",
              "      <td>0.118135</td>\n",
              "      <td>0.038843</td>\n",
              "      <td>0.023846</td>\n",
              "      <td>0.164065</td>\n",
              "      <td>0.004283</td>\n",
              "      <td>0.026130</td>\n",
              "      <td>0.136155</td>\n",
              "      <td>0.089629</td>\n",
              "      <td>0.069597</td>\n",
              "      <td>0.007847</td>\n",
              "      <td>0.050125</td>\n",
              "      <td>0.064355</td>\n",
              "      <td>0.106098</td>\n",
              "      <td>0.026106</td>\n",
              "      <td>...</td>\n",
              "      <td>0.096248</td>\n",
              "      <td>0.042711</td>\n",
              "      <td>0.058920</td>\n",
              "      <td>0.055356</td>\n",
              "      <td>0.075671</td>\n",
              "      <td>0.068572</td>\n",
              "      <td>0.033875</td>\n",
              "      <td>0.107652</td>\n",
              "      <td>0.164387</td>\n",
              "      <td>0.023132</td>\n",
              "      <td>0.057861</td>\n",
              "      <td>0.045903</td>\n",
              "      <td>0.081110</td>\n",
              "      <td>0.012094</td>\n",
              "      <td>0.124010</td>\n",
              "      <td>0.000529</td>\n",
              "      <td>0.045576</td>\n",
              "      <td>0.058535</td>\n",
              "      <td>0.056229</td>\n",
              "      <td>0.053502</td>\n",
              "      <td>0.048491</td>\n",
              "      <td>0.010273</td>\n",
              "      <td>0.045309</td>\n",
              "      <td>0.012249</td>\n",
              "      <td>0.067132</td>\n",
              "      <td>0.105056</td>\n",
              "      <td>0.027041</td>\n",
              "      <td>0.070566</td>\n",
              "      <td>0.008665</td>\n",
              "      <td>0.040411</td>\n",
              "      <td>0.057285</td>\n",
              "      <td>0.029771</td>\n",
              "      <td>0.066776</td>\n",
              "      <td>0.078849</td>\n",
              "      <td>0.125156</td>\n",
              "      <td>0.107947</td>\n",
              "      <td>0.012653</td>\n",
              "      <td>0.027744</td>\n",
              "      <td>0.049494</td>\n",
              "      <td>0.080048</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>74</th>\n",
              "      <td>0.002632</td>\n",
              "      <td>0.024216</td>\n",
              "      <td>0.048285</td>\n",
              "      <td>0.087128</td>\n",
              "      <td>0.042038</td>\n",
              "      <td>0.010513</td>\n",
              "      <td>0.029251</td>\n",
              "      <td>0.016694</td>\n",
              "      <td>0.026211</td>\n",
              "      <td>0.015956</td>\n",
              "      <td>0.034684</td>\n",
              "      <td>0.049389</td>\n",
              "      <td>0.115228</td>\n",
              "      <td>0.050018</td>\n",
              "      <td>0.081114</td>\n",
              "      <td>0.074804</td>\n",
              "      <td>0.012893</td>\n",
              "      <td>0.030471</td>\n",
              "      <td>0.044736</td>\n",
              "      <td>0.032527</td>\n",
              "      <td>0.006271</td>\n",
              "      <td>0.015995</td>\n",
              "      <td>0.035168</td>\n",
              "      <td>0.081338</td>\n",
              "      <td>0.049434</td>\n",
              "      <td>0.059180</td>\n",
              "      <td>0.149415</td>\n",
              "      <td>0.080886</td>\n",
              "      <td>0.030405</td>\n",
              "      <td>0.039385</td>\n",
              "      <td>0.022287</td>\n",
              "      <td>0.020298</td>\n",
              "      <td>0.051375</td>\n",
              "      <td>0.112943</td>\n",
              "      <td>0.027184</td>\n",
              "      <td>0.040102</td>\n",
              "      <td>0.061277</td>\n",
              "      <td>0.016533</td>\n",
              "      <td>0.102739</td>\n",
              "      <td>0.022938</td>\n",
              "      <td>...</td>\n",
              "      <td>0.027280</td>\n",
              "      <td>0.058500</td>\n",
              "      <td>0.035304</td>\n",
              "      <td>0.139660</td>\n",
              "      <td>0.091539</td>\n",
              "      <td>0.054178</td>\n",
              "      <td>0.025700</td>\n",
              "      <td>0.048190</td>\n",
              "      <td>0.086262</td>\n",
              "      <td>0.078395</td>\n",
              "      <td>0.030922</td>\n",
              "      <td>0.025863</td>\n",
              "      <td>0.036459</td>\n",
              "      <td>0.057811</td>\n",
              "      <td>0.026788</td>\n",
              "      <td>0.182402</td>\n",
              "      <td>0.036359</td>\n",
              "      <td>0.030018</td>\n",
              "      <td>0.010786</td>\n",
              "      <td>0.019969</td>\n",
              "      <td>0.031792</td>\n",
              "      <td>0.071443</td>\n",
              "      <td>0.028592</td>\n",
              "      <td>0.066892</td>\n",
              "      <td>0.134075</td>\n",
              "      <td>0.017468</td>\n",
              "      <td>0.049612</td>\n",
              "      <td>0.052509</td>\n",
              "      <td>0.025126</td>\n",
              "      <td>0.062572</td>\n",
              "      <td>0.016916</td>\n",
              "      <td>0.023243</td>\n",
              "      <td>0.016028</td>\n",
              "      <td>0.021805</td>\n",
              "      <td>0.008125</td>\n",
              "      <td>0.082297</td>\n",
              "      <td>0.034180</td>\n",
              "      <td>0.016931</td>\n",
              "      <td>0.064549</td>\n",
              "      <td>0.063312</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>136</th>\n",
              "      <td>0.002784</td>\n",
              "      <td>0.067619</td>\n",
              "      <td>0.037648</td>\n",
              "      <td>0.050142</td>\n",
              "      <td>0.143350</td>\n",
              "      <td>0.001173</td>\n",
              "      <td>0.106305</td>\n",
              "      <td>0.074862</td>\n",
              "      <td>0.004256</td>\n",
              "      <td>0.028024</td>\n",
              "      <td>0.000966</td>\n",
              "      <td>0.009075</td>\n",
              "      <td>0.063665</td>\n",
              "      <td>0.113932</td>\n",
              "      <td>0.058802</td>\n",
              "      <td>0.084826</td>\n",
              "      <td>0.088854</td>\n",
              "      <td>0.112813</td>\n",
              "      <td>0.130550</td>\n",
              "      <td>0.102214</td>\n",
              "      <td>0.076431</td>\n",
              "      <td>0.005553</td>\n",
              "      <td>0.059285</td>\n",
              "      <td>0.162131</td>\n",
              "      <td>0.000718</td>\n",
              "      <td>0.031801</td>\n",
              "      <td>0.053396</td>\n",
              "      <td>0.130998</td>\n",
              "      <td>0.154292</td>\n",
              "      <td>0.019028</td>\n",
              "      <td>0.006308</td>\n",
              "      <td>0.043880</td>\n",
              "      <td>0.067769</td>\n",
              "      <td>0.011494</td>\n",
              "      <td>0.089016</td>\n",
              "      <td>0.111874</td>\n",
              "      <td>0.061094</td>\n",
              "      <td>0.010838</td>\n",
              "      <td>0.012873</td>\n",
              "      <td>0.001033</td>\n",
              "      <td>...</td>\n",
              "      <td>0.010014</td>\n",
              "      <td>0.070881</td>\n",
              "      <td>0.061372</td>\n",
              "      <td>0.066842</td>\n",
              "      <td>0.032015</td>\n",
              "      <td>0.112050</td>\n",
              "      <td>0.091101</td>\n",
              "      <td>0.072005</td>\n",
              "      <td>0.017805</td>\n",
              "      <td>0.042754</td>\n",
              "      <td>0.027205</td>\n",
              "      <td>0.086591</td>\n",
              "      <td>0.011189</td>\n",
              "      <td>0.007150</td>\n",
              "      <td>0.056918</td>\n",
              "      <td>0.036837</td>\n",
              "      <td>0.012423</td>\n",
              "      <td>0.036697</td>\n",
              "      <td>0.039000</td>\n",
              "      <td>0.046978</td>\n",
              "      <td>0.005325</td>\n",
              "      <td>0.026437</td>\n",
              "      <td>0.057993</td>\n",
              "      <td>0.017608</td>\n",
              "      <td>0.002551</td>\n",
              "      <td>0.006409</td>\n",
              "      <td>0.017475</td>\n",
              "      <td>0.029110</td>\n",
              "      <td>0.045823</td>\n",
              "      <td>0.038732</td>\n",
              "      <td>0.037978</td>\n",
              "      <td>0.083347</td>\n",
              "      <td>0.103064</td>\n",
              "      <td>0.031890</td>\n",
              "      <td>0.003685</td>\n",
              "      <td>0.073164</td>\n",
              "      <td>0.029595</td>\n",
              "      <td>0.018614</td>\n",
              "      <td>0.070918</td>\n",
              "      <td>0.007233</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>207</th>\n",
              "      <td>0.003140</td>\n",
              "      <td>0.030000</td>\n",
              "      <td>0.205645</td>\n",
              "      <td>0.058834</td>\n",
              "      <td>0.139035</td>\n",
              "      <td>0.033643</td>\n",
              "      <td>0.226966</td>\n",
              "      <td>0.135449</td>\n",
              "      <td>0.118146</td>\n",
              "      <td>0.055816</td>\n",
              "      <td>0.063220</td>\n",
              "      <td>0.015323</td>\n",
              "      <td>0.005046</td>\n",
              "      <td>0.019112</td>\n",
              "      <td>0.050997</td>\n",
              "      <td>0.050889</td>\n",
              "      <td>0.022085</td>\n",
              "      <td>0.089347</td>\n",
              "      <td>0.026743</td>\n",
              "      <td>0.027138</td>\n",
              "      <td>0.035989</td>\n",
              "      <td>0.032926</td>\n",
              "      <td>0.078439</td>\n",
              "      <td>0.009553</td>\n",
              "      <td>0.028041</td>\n",
              "      <td>0.119333</td>\n",
              "      <td>0.128535</td>\n",
              "      <td>0.102607</td>\n",
              "      <td>0.054422</td>\n",
              "      <td>0.021228</td>\n",
              "      <td>0.029463</td>\n",
              "      <td>0.122511</td>\n",
              "      <td>0.055612</td>\n",
              "      <td>0.057632</td>\n",
              "      <td>0.061712</td>\n",
              "      <td>0.005894</td>\n",
              "      <td>0.055555</td>\n",
              "      <td>0.015544</td>\n",
              "      <td>0.057769</td>\n",
              "      <td>0.044962</td>\n",
              "      <td>...</td>\n",
              "      <td>0.027772</td>\n",
              "      <td>0.010290</td>\n",
              "      <td>0.064459</td>\n",
              "      <td>0.024208</td>\n",
              "      <td>0.066042</td>\n",
              "      <td>0.027353</td>\n",
              "      <td>0.087447</td>\n",
              "      <td>0.033020</td>\n",
              "      <td>0.076530</td>\n",
              "      <td>0.015193</td>\n",
              "      <td>0.027115</td>\n",
              "      <td>0.029434</td>\n",
              "      <td>0.020231</td>\n",
              "      <td>0.037981</td>\n",
              "      <td>0.050571</td>\n",
              "      <td>0.023759</td>\n",
              "      <td>0.098300</td>\n",
              "      <td>0.070684</td>\n",
              "      <td>0.005793</td>\n",
              "      <td>0.014859</td>\n",
              "      <td>0.023326</td>\n",
              "      <td>0.030173</td>\n",
              "      <td>0.057713</td>\n",
              "      <td>0.016309</td>\n",
              "      <td>0.041828</td>\n",
              "      <td>0.032646</td>\n",
              "      <td>0.048080</td>\n",
              "      <td>0.124892</td>\n",
              "      <td>0.030637</td>\n",
              "      <td>0.015140</td>\n",
              "      <td>0.098355</td>\n",
              "      <td>0.013979</td>\n",
              "      <td>0.007834</td>\n",
              "      <td>0.150357</td>\n",
              "      <td>0.052525</td>\n",
              "      <td>0.067486</td>\n",
              "      <td>0.016605</td>\n",
              "      <td>0.017137</td>\n",
              "      <td>0.011600</td>\n",
              "      <td>0.020820</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>191</th>\n",
              "      <td>0.003449</td>\n",
              "      <td>0.061568</td>\n",
              "      <td>0.005260</td>\n",
              "      <td>0.004555</td>\n",
              "      <td>0.019013</td>\n",
              "      <td>0.016094</td>\n",
              "      <td>0.024887</td>\n",
              "      <td>0.000456</td>\n",
              "      <td>0.033958</td>\n",
              "      <td>0.089871</td>\n",
              "      <td>0.008339</td>\n",
              "      <td>0.082366</td>\n",
              "      <td>0.044039</td>\n",
              "      <td>0.007525</td>\n",
              "      <td>0.039346</td>\n",
              "      <td>0.049901</td>\n",
              "      <td>0.002825</td>\n",
              "      <td>0.009286</td>\n",
              "      <td>0.024290</td>\n",
              "      <td>0.070493</td>\n",
              "      <td>0.080535</td>\n",
              "      <td>0.114532</td>\n",
              "      <td>0.011796</td>\n",
              "      <td>0.022147</td>\n",
              "      <td>0.032406</td>\n",
              "      <td>0.012183</td>\n",
              "      <td>0.084057</td>\n",
              "      <td>0.074515</td>\n",
              "      <td>0.074587</td>\n",
              "      <td>0.048115</td>\n",
              "      <td>0.019893</td>\n",
              "      <td>0.162330</td>\n",
              "      <td>0.006066</td>\n",
              "      <td>0.070714</td>\n",
              "      <td>0.082728</td>\n",
              "      <td>0.068218</td>\n",
              "      <td>0.097623</td>\n",
              "      <td>0.073905</td>\n",
              "      <td>0.056270</td>\n",
              "      <td>0.049874</td>\n",
              "      <td>...</td>\n",
              "      <td>0.058683</td>\n",
              "      <td>0.068480</td>\n",
              "      <td>0.051971</td>\n",
              "      <td>0.064613</td>\n",
              "      <td>0.002836</td>\n",
              "      <td>0.006808</td>\n",
              "      <td>0.098512</td>\n",
              "      <td>0.003955</td>\n",
              "      <td>0.026159</td>\n",
              "      <td>0.079515</td>\n",
              "      <td>0.037058</td>\n",
              "      <td>0.033354</td>\n",
              "      <td>0.024077</td>\n",
              "      <td>0.089150</td>\n",
              "      <td>0.059369</td>\n",
              "      <td>0.072414</td>\n",
              "      <td>0.016029</td>\n",
              "      <td>0.025240</td>\n",
              "      <td>0.053613</td>\n",
              "      <td>0.093084</td>\n",
              "      <td>0.038358</td>\n",
              "      <td>0.074322</td>\n",
              "      <td>0.012949</td>\n",
              "      <td>0.045744</td>\n",
              "      <td>0.072800</td>\n",
              "      <td>0.017823</td>\n",
              "      <td>0.035294</td>\n",
              "      <td>0.027696</td>\n",
              "      <td>0.021772</td>\n",
              "      <td>0.023376</td>\n",
              "      <td>0.066023</td>\n",
              "      <td>0.041708</td>\n",
              "      <td>0.072151</td>\n",
              "      <td>0.088280</td>\n",
              "      <td>0.120831</td>\n",
              "      <td>0.123979</td>\n",
              "      <td>0.004634</td>\n",
              "      <td>0.026790</td>\n",
              "      <td>0.120574</td>\n",
              "      <td>0.024846</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>0.003705</td>\n",
              "      <td>0.007881</td>\n",
              "      <td>0.026988</td>\n",
              "      <td>0.032334</td>\n",
              "      <td>0.145161</td>\n",
              "      <td>0.048396</td>\n",
              "      <td>0.016314</td>\n",
              "      <td>0.056855</td>\n",
              "      <td>0.072630</td>\n",
              "      <td>0.027886</td>\n",
              "      <td>0.112395</td>\n",
              "      <td>0.050353</td>\n",
              "      <td>0.044592</td>\n",
              "      <td>0.104514</td>\n",
              "      <td>0.071378</td>\n",
              "      <td>0.087963</td>\n",
              "      <td>0.061234</td>\n",
              "      <td>0.090196</td>\n",
              "      <td>0.051864</td>\n",
              "      <td>0.098532</td>\n",
              "      <td>0.169656</td>\n",
              "      <td>0.027048</td>\n",
              "      <td>0.087094</td>\n",
              "      <td>0.047703</td>\n",
              "      <td>0.080584</td>\n",
              "      <td>0.021236</td>\n",
              "      <td>0.035023</td>\n",
              "      <td>0.075397</td>\n",
              "      <td>0.000206</td>\n",
              "      <td>0.034016</td>\n",
              "      <td>0.150176</td>\n",
              "      <td>0.077243</td>\n",
              "      <td>0.093438</td>\n",
              "      <td>0.079087</td>\n",
              "      <td>0.019814</td>\n",
              "      <td>0.101186</td>\n",
              "      <td>0.038226</td>\n",
              "      <td>0.000035</td>\n",
              "      <td>0.057679</td>\n",
              "      <td>0.008940</td>\n",
              "      <td>...</td>\n",
              "      <td>0.049106</td>\n",
              "      <td>0.091358</td>\n",
              "      <td>0.123884</td>\n",
              "      <td>0.012177</td>\n",
              "      <td>0.042323</td>\n",
              "      <td>0.000235</td>\n",
              "      <td>0.132868</td>\n",
              "      <td>0.019021</td>\n",
              "      <td>0.071846</td>\n",
              "      <td>0.025741</td>\n",
              "      <td>0.043606</td>\n",
              "      <td>0.018537</td>\n",
              "      <td>0.035550</td>\n",
              "      <td>0.106118</td>\n",
              "      <td>0.020975</td>\n",
              "      <td>0.122418</td>\n",
              "      <td>0.049906</td>\n",
              "      <td>0.017658</td>\n",
              "      <td>0.060409</td>\n",
              "      <td>0.061591</td>\n",
              "      <td>0.080901</td>\n",
              "      <td>0.001718</td>\n",
              "      <td>0.098002</td>\n",
              "      <td>0.018391</td>\n",
              "      <td>0.077521</td>\n",
              "      <td>0.010853</td>\n",
              "      <td>0.004589</td>\n",
              "      <td>0.059647</td>\n",
              "      <td>0.057764</td>\n",
              "      <td>0.045831</td>\n",
              "      <td>0.045831</td>\n",
              "      <td>0.018282</td>\n",
              "      <td>0.028556</td>\n",
              "      <td>0.004267</td>\n",
              "      <td>0.051372</td>\n",
              "      <td>0.091171</td>\n",
              "      <td>0.004301</td>\n",
              "      <td>0.037859</td>\n",
              "      <td>0.070197</td>\n",
              "      <td>0.019970</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10 rows × 301 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       target         0         1  ...       297       298       299\n",
              "75   0.000694  0.023369  0.117148  ...  0.053945  0.049781  0.000569\n",
              "195  0.000909  0.101548  0.156044  ...  0.094601  0.009346  0.074856\n",
              "172  0.001375  0.058438  0.035819  ...  0.028213  0.011222  0.076948\n",
              "224  0.002248  0.024380  0.067241  ...  0.010470  0.138338  0.087211\n",
              "20   0.002429  0.000825  0.055748  ...  0.027744  0.049494  0.080048\n",
              "74   0.002632  0.024216  0.048285  ...  0.016931  0.064549  0.063312\n",
              "136  0.002784  0.067619  0.037648  ...  0.018614  0.070918  0.007233\n",
              "207  0.003140  0.030000  0.205645  ...  0.017137  0.011600  0.020820\n",
              "191  0.003449  0.061568  0.005260  ...  0.026790  0.120574  0.024846\n",
              "40   0.003705  0.007881  0.026988  ...  0.037859  0.070197  0.019970\n",
              "\n",
              "[10 rows x 301 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1qTIxf1devwE",
        "colab_type": "code",
        "outputId": "7381515e-a987-4c91-d4dc-0079fcd3ee7d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "col_var = a[(a.target>.03) &(a.target<1)].index\n",
        "col_var[:10]\n",
        "len(col_var)\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "210"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xfCWCZe7fCoF",
        "colab_type": "code",
        "outputId": "a5b40dc2-bf05-4a74-c69f-229f00354874",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(type(col_var))\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.indexes.base.Index'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9VDG4Nebe5Wq",
        "colab_type": "code",
        "outputId": "c14ac04b-9668-4817-cd11-d835096364df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "X = train\n",
        "print(np.shape(X))\n",
        "X = X[[i for i in np.array(col_var)]]\n",
        "print(np.shape(X))\n",
        "# X = X.drop(columns=['target'])\n",
        "print(np.shape(X))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(250, 301)\n",
            "(250, 210)\n",
            "(250, 210)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PyOmpcdAB2cN",
        "colab_type": "code",
        "outputId": "36b5c036-0231-4d80-bb96-78b9250c2ff1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "import copy\n",
        "y = train.target\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X1 = scaler.fit_transform(X)\n",
        "X_a = copy.deepcopy(X1)\n",
        "X1 = np.array(X1).reshape(-1,14,15,1)\n",
        "print(y.head(6))\n",
        "y = np.array(y).reshape(-1,)\n",
        "print(sum(y==1),len(y),np.shape(X1))\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0    1.0\n",
            "1    0.0\n",
            "2    1.0\n",
            "3    1.0\n",
            "4    1.0\n",
            "5    1.0\n",
            "Name: target, dtype: float64\n",
            "160 250 (250, 14, 15, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-Uo__-dddiV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CciUS5GcZb2h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dqUJhOjuZ30Y",
        "colab_type": "code",
        "outputId": "a9efc827-bf5d-48b3-db0a-3b66fcd6576e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "tf.__version__"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.0.0-beta0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WFm1jXypZ5m1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !pip install tensorflow==2.0.0-beta0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vZ5smnB8Z__c",
        "colab_type": "code",
        "outputId": "92fb51d1-f02e-4fb5-ab62-fdbec7284ca3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "tf.__version__\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.0.0-beta0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HlJUuvAuEn0Z",
        "colab_type": "text"
      },
      "source": [
        "## method1: directly use all variables and ingest into CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "97P-okDUdesx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow import keras\n",
        "model = tf.keras.models.Sequential([\n",
        "     tf.keras.layers.Conv2D(64,(3,3), activation='relu', input_shape=(14,15,1)),\n",
        "     tf.keras.layers.MaxPooling2D(2,2),\n",
        "     tf.keras.layers.Dense(64, activation='relu'), \n",
        "     tf.keras.layers.Dropout(.1), \n",
        "     tf.keras.layers.Dense(64, activation='relu'),\n",
        "     tf.keras.layers.Flatten(),\n",
        "     tf.keras.layers.Dense(16, activation='relu'),\n",
        "     tf.keras.layers.Dense(16, activation='relu'),\n",
        "     tf.keras.layers.Dropout(.2),\n",
        "     tf.keras.layers.Dropout(.3),\n",
        "     tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HFev7XWugwPS",
        "colab_type": "code",
        "outputId": "236ed467-f58e-4e73-b540-b6111ee12042",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 12, 13, 64)        640       \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 6, 6, 64)          0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 6, 6, 64)          4160      \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 6, 6, 64)          0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 6, 6, 64)          4160      \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 2304)              0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 16)                36880     \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 16)                272       \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 1)                 17        \n",
            "=================================================================\n",
            "Total params: 46,129\n",
            "Trainable params: 46,129\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HEhuOgDMf03h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer='SGD',loss='binary_crossentropy',metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fZves7Jzih0S",
        "colab_type": "code",
        "outputId": "11eef524-6b26-4c9b-d517-4212982d42f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# from sklearn.model_selection import train_test_split\n",
        "# X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=.3,random_state=12)\n",
        "# print(np.shape(X_train),np.shape(X_test))\n",
        "print(np.shape(X1),np.shape(y))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(250, 14, 15, 1) (250,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qtcABfQ1jc-f",
        "colab_type": "code",
        "outputId": "69a3f72c-9c81-4435-b252-06b258f1d086",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "history = model.fit(X1,y,epochs=200,verbose=1)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "Train on 250 samples\n",
            "Epoch 1/200\n",
            "250/250 [==============================] - 0s 2ms/sample - loss: 0.6981 - accuracy: 0.4520\n",
            "Epoch 2/200\n",
            "250/250 [==============================] - 0s 384us/sample - loss: 0.6926 - accuracy: 0.5080\n",
            "Epoch 3/200\n",
            "250/250 [==============================] - 0s 371us/sample - loss: 0.6869 - accuracy: 0.5800\n",
            "Epoch 4/200\n",
            "250/250 [==============================] - 0s 367us/sample - loss: 0.6840 - accuracy: 0.6120\n",
            "Epoch 5/200\n",
            "250/250 [==============================] - 0s 404us/sample - loss: 0.6833 - accuracy: 0.6200\n",
            "Epoch 6/200\n",
            "250/250 [==============================] - 0s 392us/sample - loss: 0.6816 - accuracy: 0.6160\n",
            "Epoch 7/200\n",
            "250/250 [==============================] - 0s 375us/sample - loss: 0.6766 - accuracy: 0.6480\n",
            "Epoch 8/200\n",
            "250/250 [==============================] - 0s 387us/sample - loss: 0.6765 - accuracy: 0.6120\n",
            "Epoch 9/200\n",
            "250/250 [==============================] - 0s 425us/sample - loss: 0.6774 - accuracy: 0.6200\n",
            "Epoch 10/200\n",
            "250/250 [==============================] - 0s 377us/sample - loss: 0.6692 - accuracy: 0.6240\n",
            "Epoch 11/200\n",
            "250/250 [==============================] - 0s 435us/sample - loss: 0.6664 - accuracy: 0.6520\n",
            "Epoch 12/200\n",
            "250/250 [==============================] - 0s 378us/sample - loss: 0.6701 - accuracy: 0.6280\n",
            "Epoch 13/200\n",
            "250/250 [==============================] - 0s 395us/sample - loss: 0.6688 - accuracy: 0.6400\n",
            "Epoch 14/200\n",
            "250/250 [==============================] - 0s 375us/sample - loss: 0.6708 - accuracy: 0.6320\n",
            "Epoch 15/200\n",
            "250/250 [==============================] - 0s 372us/sample - loss: 0.6588 - accuracy: 0.6400\n",
            "Epoch 16/200\n",
            "250/250 [==============================] - 0s 365us/sample - loss: 0.6625 - accuracy: 0.6360\n",
            "Epoch 17/200\n",
            "250/250 [==============================] - 0s 369us/sample - loss: 0.6618 - accuracy: 0.6360\n",
            "Epoch 18/200\n",
            "250/250 [==============================] - 0s 387us/sample - loss: 0.6623 - accuracy: 0.6520\n",
            "Epoch 19/200\n",
            "250/250 [==============================] - 0s 353us/sample - loss: 0.6652 - accuracy: 0.6440\n",
            "Epoch 20/200\n",
            "250/250 [==============================] - 0s 371us/sample - loss: 0.6527 - accuracy: 0.6360\n",
            "Epoch 21/200\n",
            "250/250 [==============================] - 0s 362us/sample - loss: 0.6560 - accuracy: 0.6440\n",
            "Epoch 22/200\n",
            "250/250 [==============================] - 0s 403us/sample - loss: 0.6566 - accuracy: 0.6360\n",
            "Epoch 23/200\n",
            "250/250 [==============================] - 0s 369us/sample - loss: 0.6672 - accuracy: 0.6360\n",
            "Epoch 24/200\n",
            "250/250 [==============================] - 0s 462us/sample - loss: 0.6600 - accuracy: 0.6360\n",
            "Epoch 25/200\n",
            "250/250 [==============================] - 0s 364us/sample - loss: 0.6503 - accuracy: 0.6440\n",
            "Epoch 26/200\n",
            "250/250 [==============================] - 0s 378us/sample - loss: 0.6567 - accuracy: 0.6360\n",
            "Epoch 27/200\n",
            "250/250 [==============================] - 0s 377us/sample - loss: 0.6491 - accuracy: 0.6400\n",
            "Epoch 28/200\n",
            "250/250 [==============================] - 0s 375us/sample - loss: 0.6611 - accuracy: 0.6400\n",
            "Epoch 29/200\n",
            "250/250 [==============================] - 0s 355us/sample - loss: 0.6583 - accuracy: 0.6400\n",
            "Epoch 30/200\n",
            "250/250 [==============================] - 0s 373us/sample - loss: 0.6523 - accuracy: 0.6400\n",
            "Epoch 31/200\n",
            "250/250 [==============================] - 0s 383us/sample - loss: 0.6564 - accuracy: 0.6400\n",
            "Epoch 32/200\n",
            "250/250 [==============================] - 0s 400us/sample - loss: 0.6526 - accuracy: 0.6400\n",
            "Epoch 33/200\n",
            "250/250 [==============================] - 0s 393us/sample - loss: 0.6509 - accuracy: 0.6400\n",
            "Epoch 34/200\n",
            "250/250 [==============================] - 0s 362us/sample - loss: 0.6536 - accuracy: 0.6400\n",
            "Epoch 35/200\n",
            "250/250 [==============================] - 0s 363us/sample - loss: 0.6465 - accuracy: 0.6400\n",
            "Epoch 36/200\n",
            "250/250 [==============================] - 0s 359us/sample - loss: 0.6460 - accuracy: 0.6400\n",
            "Epoch 37/200\n",
            "250/250 [==============================] - 0s 358us/sample - loss: 0.6574 - accuracy: 0.6400\n",
            "Epoch 38/200\n",
            "250/250 [==============================] - 0s 376us/sample - loss: 0.6459 - accuracy: 0.6360\n",
            "Epoch 39/200\n",
            "250/250 [==============================] - 0s 365us/sample - loss: 0.6519 - accuracy: 0.6320\n",
            "Epoch 40/200\n",
            "250/250 [==============================] - 0s 367us/sample - loss: 0.6491 - accuracy: 0.6400\n",
            "Epoch 41/200\n",
            "250/250 [==============================] - 0s 362us/sample - loss: 0.6434 - accuracy: 0.6400\n",
            "Epoch 42/200\n",
            "250/250 [==============================] - 0s 357us/sample - loss: 0.6445 - accuracy: 0.6400\n",
            "Epoch 43/200\n",
            "250/250 [==============================] - 0s 408us/sample - loss: 0.6328 - accuracy: 0.6400\n",
            "Epoch 44/200\n",
            "250/250 [==============================] - 0s 373us/sample - loss: 0.6400 - accuracy: 0.6400\n",
            "Epoch 45/200\n",
            "250/250 [==============================] - 0s 396us/sample - loss: 0.6366 - accuracy: 0.6440\n",
            "Epoch 46/200\n",
            "250/250 [==============================] - 0s 387us/sample - loss: 0.6432 - accuracy: 0.6440\n",
            "Epoch 47/200\n",
            "250/250 [==============================] - 0s 388us/sample - loss: 0.6338 - accuracy: 0.6360\n",
            "Epoch 48/200\n",
            "250/250 [==============================] - 0s 404us/sample - loss: 0.6340 - accuracy: 0.6400\n",
            "Epoch 49/200\n",
            "250/250 [==============================] - 0s 408us/sample - loss: 0.6309 - accuracy: 0.6400\n",
            "Epoch 50/200\n",
            "250/250 [==============================] - 0s 430us/sample - loss: 0.6452 - accuracy: 0.6440\n",
            "Epoch 51/200\n",
            "250/250 [==============================] - 0s 388us/sample - loss: 0.6419 - accuracy: 0.6440\n",
            "Epoch 52/200\n",
            "250/250 [==============================] - 0s 386us/sample - loss: 0.6376 - accuracy: 0.6440\n",
            "Epoch 53/200\n",
            "250/250 [==============================] - 0s 419us/sample - loss: 0.6310 - accuracy: 0.6400\n",
            "Epoch 54/200\n",
            "250/250 [==============================] - 0s 364us/sample - loss: 0.6458 - accuracy: 0.6400\n",
            "Epoch 55/200\n",
            "250/250 [==============================] - 0s 370us/sample - loss: 0.6395 - accuracy: 0.6400\n",
            "Epoch 56/200\n",
            "250/250 [==============================] - 0s 371us/sample - loss: 0.6380 - accuracy: 0.6400\n",
            "Epoch 57/200\n",
            "250/250 [==============================] - 0s 364us/sample - loss: 0.6345 - accuracy: 0.6400\n",
            "Epoch 58/200\n",
            "250/250 [==============================] - 0s 360us/sample - loss: 0.6367 - accuracy: 0.6400\n",
            "Epoch 59/200\n",
            "250/250 [==============================] - 0s 347us/sample - loss: 0.6337 - accuracy: 0.6400\n",
            "Epoch 60/200\n",
            "250/250 [==============================] - 0s 341us/sample - loss: 0.6293 - accuracy: 0.6440\n",
            "Epoch 61/200\n",
            "250/250 [==============================] - 0s 359us/sample - loss: 0.6207 - accuracy: 0.6360\n",
            "Epoch 62/200\n",
            "250/250 [==============================] - 0s 357us/sample - loss: 0.6293 - accuracy: 0.6400\n",
            "Epoch 63/200\n",
            "250/250 [==============================] - 0s 353us/sample - loss: 0.6264 - accuracy: 0.6400\n",
            "Epoch 64/200\n",
            "250/250 [==============================] - 0s 405us/sample - loss: 0.6234 - accuracy: 0.6440\n",
            "Epoch 65/200\n",
            "250/250 [==============================] - 0s 391us/sample - loss: 0.6206 - accuracy: 0.6520\n",
            "Epoch 66/200\n",
            "250/250 [==============================] - 0s 370us/sample - loss: 0.6226 - accuracy: 0.6400\n",
            "Epoch 67/200\n",
            "250/250 [==============================] - 0s 410us/sample - loss: 0.6184 - accuracy: 0.6440\n",
            "Epoch 68/200\n",
            "250/250 [==============================] - 0s 402us/sample - loss: 0.6221 - accuracy: 0.6440\n",
            "Epoch 69/200\n",
            "250/250 [==============================] - 0s 360us/sample - loss: 0.6074 - accuracy: 0.6640\n",
            "Epoch 70/200\n",
            "250/250 [==============================] - 0s 372us/sample - loss: 0.6036 - accuracy: 0.6480\n",
            "Epoch 71/200\n",
            "250/250 [==============================] - 0s 371us/sample - loss: 0.6177 - accuracy: 0.6600\n",
            "Epoch 72/200\n",
            "250/250 [==============================] - 0s 390us/sample - loss: 0.6136 - accuracy: 0.6400\n",
            "Epoch 73/200\n",
            "250/250 [==============================] - 0s 395us/sample - loss: 0.6069 - accuracy: 0.6520\n",
            "Epoch 74/200\n",
            "250/250 [==============================] - 0s 426us/sample - loss: 0.6063 - accuracy: 0.6480\n",
            "Epoch 75/200\n",
            "250/250 [==============================] - 0s 364us/sample - loss: 0.6038 - accuracy: 0.6680\n",
            "Epoch 76/200\n",
            "250/250 [==============================] - 0s 360us/sample - loss: 0.6072 - accuracy: 0.6440\n",
            "Epoch 77/200\n",
            "250/250 [==============================] - 0s 370us/sample - loss: 0.6066 - accuracy: 0.6640\n",
            "Epoch 78/200\n",
            "250/250 [==============================] - 0s 376us/sample - loss: 0.5982 - accuracy: 0.6640\n",
            "Epoch 79/200\n",
            "250/250 [==============================] - 0s 366us/sample - loss: 0.5864 - accuracy: 0.6760\n",
            "Epoch 80/200\n",
            "250/250 [==============================] - 0s 372us/sample - loss: 0.5954 - accuracy: 0.6560\n",
            "Epoch 81/200\n",
            "250/250 [==============================] - 0s 362us/sample - loss: 0.5999 - accuracy: 0.6520\n",
            "Epoch 82/200\n",
            "250/250 [==============================] - 0s 372us/sample - loss: 0.5971 - accuracy: 0.6760\n",
            "Epoch 83/200\n",
            "250/250 [==============================] - 0s 365us/sample - loss: 0.5951 - accuracy: 0.6600\n",
            "Epoch 84/200\n",
            "250/250 [==============================] - 0s 378us/sample - loss: 0.5866 - accuracy: 0.6640\n",
            "Epoch 85/200\n",
            "250/250 [==============================] - 0s 424us/sample - loss: 0.6005 - accuracy: 0.6640\n",
            "Epoch 86/200\n",
            "250/250 [==============================] - 0s 383us/sample - loss: 0.5794 - accuracy: 0.6600\n",
            "Epoch 87/200\n",
            "250/250 [==============================] - 0s 363us/sample - loss: 0.5818 - accuracy: 0.6720\n",
            "Epoch 88/200\n",
            "250/250 [==============================] - 0s 364us/sample - loss: 0.5750 - accuracy: 0.6680\n",
            "Epoch 89/200\n",
            "250/250 [==============================] - 0s 370us/sample - loss: 0.5713 - accuracy: 0.6720\n",
            "Epoch 90/200\n",
            "250/250 [==============================] - 0s 361us/sample - loss: 0.5812 - accuracy: 0.6720\n",
            "Epoch 91/200\n",
            "250/250 [==============================] - 0s 355us/sample - loss: 0.5934 - accuracy: 0.6920\n",
            "Epoch 92/200\n",
            "250/250 [==============================] - 0s 339us/sample - loss: 0.5822 - accuracy: 0.6720\n",
            "Epoch 93/200\n",
            "250/250 [==============================] - 0s 354us/sample - loss: 0.5769 - accuracy: 0.6920\n",
            "Epoch 94/200\n",
            "250/250 [==============================] - 0s 362us/sample - loss: 0.5688 - accuracy: 0.7120\n",
            "Epoch 95/200\n",
            "250/250 [==============================] - 0s 365us/sample - loss: 0.5845 - accuracy: 0.6720\n",
            "Epoch 96/200\n",
            "250/250 [==============================] - 0s 411us/sample - loss: 0.5551 - accuracy: 0.6960\n",
            "Epoch 97/200\n",
            "250/250 [==============================] - 0s 377us/sample - loss: 0.5770 - accuracy: 0.7120\n",
            "Epoch 98/200\n",
            "250/250 [==============================] - 0s 377us/sample - loss: 0.5590 - accuracy: 0.7040\n",
            "Epoch 99/200\n",
            "250/250 [==============================] - 0s 379us/sample - loss: 0.5417 - accuracy: 0.7680\n",
            "Epoch 100/200\n",
            "250/250 [==============================] - 0s 360us/sample - loss: 0.5463 - accuracy: 0.6920\n",
            "Epoch 101/200\n",
            "250/250 [==============================] - 0s 374us/sample - loss: 0.5465 - accuracy: 0.7240\n",
            "Epoch 102/200\n",
            "250/250 [==============================] - 0s 367us/sample - loss: 0.5423 - accuracy: 0.6880\n",
            "Epoch 103/200\n",
            "250/250 [==============================] - 0s 361us/sample - loss: 0.5494 - accuracy: 0.7520\n",
            "Epoch 104/200\n",
            "250/250 [==============================] - 0s 409us/sample - loss: 0.5542 - accuracy: 0.7200\n",
            "Epoch 105/200\n",
            "250/250 [==============================] - 0s 374us/sample - loss: 0.5318 - accuracy: 0.7240\n",
            "Epoch 106/200\n",
            "250/250 [==============================] - 0s 423us/sample - loss: 0.5283 - accuracy: 0.7240\n",
            "Epoch 107/200\n",
            "250/250 [==============================] - 0s 366us/sample - loss: 0.5367 - accuracy: 0.7320\n",
            "Epoch 108/200\n",
            "250/250 [==============================] - 0s 372us/sample - loss: 0.5352 - accuracy: 0.7640\n",
            "Epoch 109/200\n",
            "250/250 [==============================] - 0s 354us/sample - loss: 0.5302 - accuracy: 0.7600\n",
            "Epoch 110/200\n",
            "250/250 [==============================] - 0s 403us/sample - loss: 0.5066 - accuracy: 0.7720\n",
            "Epoch 111/200\n",
            "250/250 [==============================] - 0s 357us/sample - loss: 0.5168 - accuracy: 0.7360\n",
            "Epoch 112/200\n",
            "250/250 [==============================] - 0s 363us/sample - loss: 0.5053 - accuracy: 0.7600\n",
            "Epoch 113/200\n",
            "250/250 [==============================] - 0s 346us/sample - loss: 0.5013 - accuracy: 0.7560\n",
            "Epoch 114/200\n",
            "250/250 [==============================] - 0s 356us/sample - loss: 0.5116 - accuracy: 0.7560\n",
            "Epoch 115/200\n",
            "250/250 [==============================] - 0s 353us/sample - loss: 0.5018 - accuracy: 0.7760\n",
            "Epoch 116/200\n",
            "250/250 [==============================] - 0s 339us/sample - loss: 0.4820 - accuracy: 0.7680\n",
            "Epoch 117/200\n",
            "250/250 [==============================] - 0s 439us/sample - loss: 0.4947 - accuracy: 0.8000\n",
            "Epoch 118/200\n",
            "250/250 [==============================] - 0s 401us/sample - loss: 0.4708 - accuracy: 0.8080\n",
            "Epoch 119/200\n",
            "250/250 [==============================] - 0s 435us/sample - loss: 0.4844 - accuracy: 0.7840\n",
            "Epoch 120/200\n",
            "250/250 [==============================] - 0s 368us/sample - loss: 0.4783 - accuracy: 0.7800\n",
            "Epoch 121/200\n",
            "250/250 [==============================] - 0s 365us/sample - loss: 0.4634 - accuracy: 0.8120\n",
            "Epoch 122/200\n",
            "250/250 [==============================] - 0s 365us/sample - loss: 0.4707 - accuracy: 0.7840\n",
            "Epoch 123/200\n",
            "250/250 [==============================] - 0s 368us/sample - loss: 0.4534 - accuracy: 0.8000\n",
            "Epoch 124/200\n",
            "250/250 [==============================] - 0s 382us/sample - loss: 0.4295 - accuracy: 0.8360\n",
            "Epoch 125/200\n",
            "250/250 [==============================] - 0s 391us/sample - loss: 0.4587 - accuracy: 0.7880\n",
            "Epoch 126/200\n",
            "250/250 [==============================] - 0s 375us/sample - loss: 0.4758 - accuracy: 0.7600\n",
            "Epoch 127/200\n",
            "250/250 [==============================] - 0s 436us/sample - loss: 0.4290 - accuracy: 0.8280\n",
            "Epoch 128/200\n",
            "250/250 [==============================] - 0s 379us/sample - loss: 0.4467 - accuracy: 0.7880\n",
            "Epoch 129/200\n",
            "250/250 [==============================] - 0s 360us/sample - loss: 0.4201 - accuracy: 0.8200\n",
            "Epoch 130/200\n",
            "250/250 [==============================] - 0s 366us/sample - loss: 0.4382 - accuracy: 0.8040\n",
            "Epoch 131/200\n",
            "250/250 [==============================] - 0s 381us/sample - loss: 0.4563 - accuracy: 0.7760\n",
            "Epoch 132/200\n",
            "250/250 [==============================] - 0s 369us/sample - loss: 0.4406 - accuracy: 0.8120\n",
            "Epoch 133/200\n",
            "250/250 [==============================] - 0s 353us/sample - loss: 0.4173 - accuracy: 0.8080\n",
            "Epoch 134/200\n",
            "250/250 [==============================] - 0s 348us/sample - loss: 0.4044 - accuracy: 0.8320\n",
            "Epoch 135/200\n",
            "250/250 [==============================] - 0s 375us/sample - loss: 0.4078 - accuracy: 0.8200\n",
            "Epoch 136/200\n",
            "250/250 [==============================] - 0s 380us/sample - loss: 0.4032 - accuracy: 0.8520\n",
            "Epoch 137/200\n",
            "250/250 [==============================] - 0s 384us/sample - loss: 0.4048 - accuracy: 0.8320\n",
            "Epoch 138/200\n",
            "250/250 [==============================] - 0s 414us/sample - loss: 0.4053 - accuracy: 0.8400\n",
            "Epoch 139/200\n",
            "250/250 [==============================] - 0s 397us/sample - loss: 0.3696 - accuracy: 0.8480\n",
            "Epoch 140/200\n",
            "250/250 [==============================] - 0s 403us/sample - loss: 0.3877 - accuracy: 0.8520\n",
            "Epoch 141/200\n",
            "250/250 [==============================] - 0s 353us/sample - loss: 0.3777 - accuracy: 0.8440\n",
            "Epoch 142/200\n",
            "250/250 [==============================] - 0s 369us/sample - loss: 0.3729 - accuracy: 0.8720\n",
            "Epoch 143/200\n",
            "250/250 [==============================] - 0s 347us/sample - loss: 0.3745 - accuracy: 0.8480\n",
            "Epoch 144/200\n",
            "250/250 [==============================] - 0s 345us/sample - loss: 0.3849 - accuracy: 0.8280\n",
            "Epoch 145/200\n",
            "250/250 [==============================] - 0s 338us/sample - loss: 0.3677 - accuracy: 0.8440\n",
            "Epoch 146/200\n",
            "250/250 [==============================] - 0s 360us/sample - loss: 0.3501 - accuracy: 0.8640\n",
            "Epoch 147/200\n",
            "250/250 [==============================] - 0s 351us/sample - loss: 0.3344 - accuracy: 0.8840\n",
            "Epoch 148/200\n",
            "250/250 [==============================] - 0s 399us/sample - loss: 0.3385 - accuracy: 0.8480\n",
            "Epoch 149/200\n",
            "250/250 [==============================] - 0s 408us/sample - loss: 0.3661 - accuracy: 0.8320\n",
            "Epoch 150/200\n",
            "250/250 [==============================] - 0s 364us/sample - loss: 0.3307 - accuracy: 0.8520\n",
            "Epoch 151/200\n",
            "250/250 [==============================] - 0s 382us/sample - loss: 0.3950 - accuracy: 0.8000\n",
            "Epoch 152/200\n",
            "250/250 [==============================] - 0s 365us/sample - loss: 0.3521 - accuracy: 0.8640\n",
            "Epoch 153/200\n",
            "250/250 [==============================] - 0s 355us/sample - loss: 0.3353 - accuracy: 0.8480\n",
            "Epoch 154/200\n",
            "250/250 [==============================] - 0s 351us/sample - loss: 0.3325 - accuracy: 0.8920\n",
            "Epoch 155/200\n",
            "250/250 [==============================] - 0s 365us/sample - loss: 0.3188 - accuracy: 0.8520\n",
            "Epoch 156/200\n",
            "250/250 [==============================] - 0s 349us/sample - loss: 0.3138 - accuracy: 0.8880\n",
            "Epoch 157/200\n",
            "250/250 [==============================] - 0s 346us/sample - loss: 0.3110 - accuracy: 0.8840\n",
            "Epoch 158/200\n",
            "250/250 [==============================] - 0s 370us/sample - loss: 0.2951 - accuracy: 0.9040\n",
            "Epoch 159/200\n",
            "250/250 [==============================] - 0s 397us/sample - loss: 0.2984 - accuracy: 0.8960\n",
            "Epoch 160/200\n",
            "250/250 [==============================] - 0s 411us/sample - loss: 0.2778 - accuracy: 0.9080\n",
            "Epoch 161/200\n",
            "250/250 [==============================] - 0s 350us/sample - loss: 0.2953 - accuracy: 0.8760\n",
            "Epoch 162/200\n",
            "250/250 [==============================] - 0s 367us/sample - loss: 0.2888 - accuracy: 0.8800\n",
            "Epoch 163/200\n",
            "250/250 [==============================] - 0s 354us/sample - loss: 0.2820 - accuracy: 0.8800\n",
            "Epoch 164/200\n",
            "250/250 [==============================] - 0s 363us/sample - loss: 0.3101 - accuracy: 0.8680\n",
            "Epoch 165/200\n",
            "250/250 [==============================] - 0s 347us/sample - loss: 0.2553 - accuracy: 0.9200\n",
            "Epoch 166/200\n",
            "250/250 [==============================] - 0s 346us/sample - loss: 0.2668 - accuracy: 0.8960\n",
            "Epoch 167/200\n",
            "250/250 [==============================] - 0s 359us/sample - loss: 0.2877 - accuracy: 0.8840\n",
            "Epoch 168/200\n",
            "250/250 [==============================] - 0s 349us/sample - loss: 0.2808 - accuracy: 0.8760\n",
            "Epoch 169/200\n",
            "250/250 [==============================] - 0s 343us/sample - loss: 0.2590 - accuracy: 0.9200\n",
            "Epoch 170/200\n",
            "250/250 [==============================] - 0s 361us/sample - loss: 0.2742 - accuracy: 0.8960\n",
            "Epoch 171/200\n",
            "250/250 [==============================] - 0s 396us/sample - loss: 0.2530 - accuracy: 0.8920\n",
            "Epoch 172/200\n",
            "250/250 [==============================] - 0s 339us/sample - loss: 0.2905 - accuracy: 0.9040\n",
            "Epoch 173/200\n",
            "250/250 [==============================] - 0s 338us/sample - loss: 0.2796 - accuracy: 0.8880\n",
            "Epoch 174/200\n",
            "250/250 [==============================] - 0s 341us/sample - loss: 0.2442 - accuracy: 0.9200\n",
            "Epoch 175/200\n",
            "250/250 [==============================] - 0s 336us/sample - loss: 0.2366 - accuracy: 0.9120\n",
            "Epoch 176/200\n",
            "250/250 [==============================] - 0s 339us/sample - loss: 0.2460 - accuracy: 0.9080\n",
            "Epoch 177/200\n",
            "250/250 [==============================] - 0s 333us/sample - loss: 0.2222 - accuracy: 0.9360\n",
            "Epoch 178/200\n",
            "250/250 [==============================] - 0s 338us/sample - loss: 0.2372 - accuracy: 0.9080\n",
            "Epoch 179/200\n",
            "250/250 [==============================] - 0s 365us/sample - loss: 0.2445 - accuracy: 0.9080\n",
            "Epoch 180/200\n",
            "250/250 [==============================] - 0s 343us/sample - loss: 0.2533 - accuracy: 0.8840\n",
            "Epoch 181/200\n",
            "250/250 [==============================] - 0s 328us/sample - loss: 0.1915 - accuracy: 0.9400\n",
            "Epoch 182/200\n",
            "250/250 [==============================] - 0s 369us/sample - loss: 0.2240 - accuracy: 0.9240\n",
            "Epoch 183/200\n",
            "250/250 [==============================] - 0s 340us/sample - loss: 0.2533 - accuracy: 0.8960\n",
            "Epoch 184/200\n",
            "250/250 [==============================] - 0s 336us/sample - loss: 0.1872 - accuracy: 0.9280\n",
            "Epoch 185/200\n",
            "250/250 [==============================] - 0s 333us/sample - loss: 0.1922 - accuracy: 0.9560\n",
            "Epoch 186/200\n",
            "250/250 [==============================] - 0s 361us/sample - loss: 0.2292 - accuracy: 0.8960\n",
            "Epoch 187/200\n",
            "250/250 [==============================] - 0s 331us/sample - loss: 0.2443 - accuracy: 0.9200\n",
            "Epoch 188/200\n",
            "250/250 [==============================] - 0s 344us/sample - loss: 0.1929 - accuracy: 0.9440\n",
            "Epoch 189/200\n",
            "250/250 [==============================] - 0s 345us/sample - loss: 0.1946 - accuracy: 0.9280\n",
            "Epoch 190/200\n",
            "250/250 [==============================] - 0s 353us/sample - loss: 0.1926 - accuracy: 0.9520\n",
            "Epoch 191/200\n",
            "250/250 [==============================] - 0s 354us/sample - loss: 0.2049 - accuracy: 0.9200\n",
            "Epoch 192/200\n",
            "250/250 [==============================] - 0s 335us/sample - loss: 0.1755 - accuracy: 0.9480\n",
            "Epoch 193/200\n",
            "250/250 [==============================] - 0s 337us/sample - loss: 0.1895 - accuracy: 0.9560\n",
            "Epoch 194/200\n",
            "250/250 [==============================] - 0s 386us/sample - loss: 0.1653 - accuracy: 0.9640\n",
            "Epoch 195/200\n",
            "250/250 [==============================] - 0s 371us/sample - loss: 0.1971 - accuracy: 0.9280\n",
            "Epoch 196/200\n",
            "250/250 [==============================] - 0s 340us/sample - loss: 0.1993 - accuracy: 0.9200\n",
            "Epoch 197/200\n",
            "250/250 [==============================] - 0s 338us/sample - loss: 0.1634 - accuracy: 0.9560\n",
            "Epoch 198/200\n",
            "250/250 [==============================] - 0s 337us/sample - loss: 0.1498 - accuracy: 0.9560\n",
            "Epoch 199/200\n",
            "250/250 [==============================] - 0s 344us/sample - loss: 0.1624 - accuracy: 0.9480\n",
            "Epoch 200/200\n",
            "250/250 [==============================] - 0s 360us/sample - loss: 0.1530 - accuracy: 0.9560\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YGUeJMXDDLut",
        "colab_type": "code",
        "outputId": "e9bb72f7-ed0b-42f2-9c89-a51d13f47ee4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        }
      },
      "source": [
        "plt.plot(range(len(history.history['accuracy'])),history.history['accuracy'])"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7fdb458d22e8>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzt3Xd8nNWd7/HPmZE0GrVRL5ZsyUXu\nxjY4tumEQDAkwIYkBDaFbApppO+S5OYmS3LvlpRNluwlm0DCJqQ5bEhxWAOhhWob27hjbMuyZavX\nmZE0febcP55nRjPSqNiozej3fr38Qnr0zOhoJL766fec5xyltUYIIUR6scz0AIQQQkw+CXchhEhD\nEu5CCJGGJNyFECINSbgLIUQaknAXQog0JOEuhBBpSMJdCCHSkIS7EEKkoYyZ+sSlpaW6rq5upj69\nEEKkpL1793ZrrcvGO2/Gwr2uro49e/bM1KcXQoiUpJRqmsh50pYRQog0JOEuhBBpSMJdCCHSkIS7\nEEKkIQl3IYRIQxLuQgiRhiTchRAiDUm4CyHEJPAFw/x61xlC4chMDwWQcBdCiElx//ON/K8/HOKV\nU70zPRRAwl0IId6wfl+Qn754CoBWl2+GR2OQcBdCiGF2nOzB5Q1O+PyHdjTFzm9zemPHG7sGeGjH\naX696wyeQAitNT9+7iQtcedMlRlbW0YIIWYjlyfIe3+yky9tWc7Hrlw87vkD/hAPvNDIm5eVcaDZ\nRZt7qHL/5+1HeepoJwAtTg8XLyrlXx57nRxbBu/fXDtlXwNI5S6EEAlO9QwS0dAzGEj68UPNLt73\nk134gmEAfrGjCacnyGevWUplQTbtcW2Z5j4vVywt4/rVlfz85Sa+/cTrVDmyuXVDzZR/HRLuQggR\np6lnEDAq+GSePNrBiw3dNHQOMGhW7VcuLWPd/EKqHNm0xYV7u9vHgmI7n72mngF/iIPNLj5x1WJs\nGdYp/zok3IUQIs7pbg/AqD33hs5+wKjKn3m9k97BAJ+8ymjfVBVm0+4y+uneQBinJ0iVw87yygJu\nXDuP6kI7t26YPw1fhfTchRAiQaxyjwv3A2edPLSjiW+9cw0nOgYAaO7zMOg3WjNr5xcCUOWw0+cJ\n4guGaTd771WObAC+++4L8IciZGdOfdUOUrkLIeaws70etNYJx04lCfdHD7byyKvNHGh2cqrb+Hhz\nn5emnkHmObJjgV1ZYAR5m8sXmzVTaYa7LcNKQXbm1H5BcSTchRBzUqvTy5XfeZbth9oTjjf1jGzL\nHDer9a2vnCUUMX4ZtDi9nO4ZpLYkN3ZetEpvc3ljvfcqh33qvogxSLgLIdKayxvka388zKA/lHC8\nzeUlouG5450J5/YOBrBaFO64cG/oNMJ924FWwKjQjcrdQ11pTuy8aJXe7vLF2jLRan66SbgLIdLa\nc8e7+MXOJnac7Ek47jRnw+xsHFou4IxZtdeX59HvDxGOaAb8IVqcXjIsCn8oglJwxdJSGrsG6BkM\nUJdQuRtVepvLR5vLS1FOJvas6emxDyfhLoRIa01mj/y02UuP6jPD/UyvhzZzhku03762xrhA2u8L\nxqr261ZVAjC/KIf68nz8IWOBsPi2jD3LSmFOplG5u3xUzlBLBiTchRBp7rRZjUd76VFOz9BNSrvM\n6j36i2BNjQMw2jQnOoypj+/dvAAwqvrqoqHQjm/LgNGGaXN5aXX6Yj34mSDhLoRIa9GKfXjl7vIG\nUQoKsjPY2Wi0bM70eijPt8X65C6vUblnWS1srCvm5nXzeNsFVdTEhXttcW7C866a52DHyR7O9Hpi\nPfiZIPPchRBpLTpvfWTlHsRhz2RDbRGvnDYq9+Y+LzVFdhw5xpRFlzfIic4BFpXlkmG1cO9t6wHo\nNZcmqCzIHtFT/9iVi/j9vma0hqoZupgKUrkLIWapEx39PH+8a8TxVqeXxw61jfq4vx7r5EirCzB6\n5t0DAXKyrDT3eQiEhjbScHqDFNozWV5ZwJkeD8FwhGanh5qiHBz2oXA/3tFPfUV+wucoysnEnmml\ntiSxJQOwtCKfG1ZXAcxo5S7hLoSYle57toFP/2bfiJuMHtrRxCd+9Sr9vuTLA9z9u4N8+tf7CEd0\nrFq/ZHEpEW3cVRrl9ARw5GRRW5JDKKI52+uhzemjpsgeu9mo3eWjuc9LfXlewudQSnH1inKuWlae\ndAyfu6aemiI768w7V2eCtGWEELNSz2AAlzdI14Cf8vyhCriz35g/frJrcER4+oJhOvv9dPb7+fOB\nVjKsCoArl5Xx1NEOmno8LCozgtrlDVKcm0VdqdEzf+VUL6GITqjc951xAowId4D7/vbCUcdeX5HP\ni1+6+ny/9EkhlbsQYlbqM2ezRNdyieoeiB7vH/GYVvOWf4uCHzxzgpOdRr/9qqVlQOJFVafHaMtE\nWysvNnQDUF1kJzvTQpbVwp4moxc/vC2TCiTchRCzUt+g0XYZHuLd/X7jeOfAiMc09xnh/neXLqSx\na5Af/rWB8nwbNUV28mwZCRdVnZ4AhTlZlOXZyM2y8rJ5k1NNkR2lFAX2TDrcfjKtKmlvfbabULgr\npbYopY4ppRqUUl9O8vFapdTTSqmDSqm/KqWmfiV6IURKO909yC0/fCnWZhkuVrl3Dq/czXBPUrlH\nw/1Dly3k7i3L8Ici1JXkopQR0Ft3n+HN3/0rDZ0DuH0hHPZM82O5sRkw1YXGNEeH3ehaLyzNJdOa\nenXwuCNWSlmB+4DrgZXA7UqplcNO+y7wkNb6AuCbwL9M9kCFEKkvFI6wx5x2+PCes7x6xhnra8fz\nBcN4AsZyuvHhHono2A5J8cePd/TTNxiguc9DhkVRkW/jk1ct4dvvuoC7rl4CwOeuWcqWVZWc6h7k\niSPGYmGF5pTH6I1IZfm22AqP0b57fXnqtWRgYhdUNwINWutGAKXUVuBm4LW4c1YCXzDffhb442QO\nUgiRHv7nUBuf3bqfn3xgA9vN6YwtfSM3i46u+5JltXCiox+tNUopnN4g4YimJDeL5j4vnkAIheKW\nH77MDWsq8YciVDqyyTAr7fiNMa5dWcE1K8p58rUOXjll/IKJhnt0CYFo1Q5x4V4x8mJqKpjI3xrV\nwNm495vNY/EOALeYb78DyFdKlbzx4Qkh0km0Sv/anw7HlgVoThLu0RbJuvmF9HmCsWo92pLZvNiI\nl5Odgzx3vJMBf4iXT/bEbkIajVKKJeV5vNrUB0ChPQuAOrOnHv/YVK/cJ6uR9PfAlUqpfcCVQAsQ\nHn6SUupOpdQepdSerq6RNycIIdLbkVYXWVYLbS4fFgUVBbaEuedR0XVf3rSwCBiaMRO9mHrxIiPc\nj3f08z/meuzNfV6OtLqoKRr74ueS8nz6zeV/o3eiRld2jH9swRyo3FuA+E3/asxjMVrrVq31LVrr\n9cBXzWMjGmla6/u11hu01hvKysrewLCFEKkmHNEcaXXz7g011JbkcFl9GSurCmhxJqnczXDfbIb4\nwWYjTrrMyn1DXRG5WVYe2nGaZ452cOECY767LxgZs3IHWBoX1kU5RuW+uDyPTKtK+FhNkZ18W0bC\nkr6pZCLhvhuoV0otVEplAbcB2+JPUEqVKqWiz/UV4MHJHaYQIpU0dPbzr4+9nnB36anuATyBMOsX\nFPH7T1zCf9y+npqinKRtmehyvMsq81lZVcDj5gXQ6Bz3yoJsvveedRxpdTMYCPPZa5bG+ufxffNk\n4ivxQrM6L82z8ezfX8XN64Y6zndcUseTX7iSrIzUmykDEwh3rXUIuAt4AjgKPKy1PqKU+qZS6ibz\ntKuAY0qp40AF8E9TNF4hRAp49GAbP3ruJF39frTWOD0BDre4AVhdXUBJng2HPZPqIjsubxD3sKUE\n+swee1FOFm+7oIp9Z5y0Or10Dxjzzh32TK5bVcmP3ncRt26o4ZLFJWysKwYYty0T30OPtl6ij7Na\nVOx9W4Z1RteGeaMmtPyA1no7sH3Ysa/Hvf074HeTOzQhRKqKXvjscPt55XQvn926n+WV+WRnWlhS\nltj6AGPGTEHVUND2eQLk2zLItFq4fnUl33niGI8fbqe7309Jrg2ljBC+ZmUF16ysAOCy+lKePNox\nYn314aoL7dgzrWRaVUKYp5vU/HtDCDHtQuEI9z51IlZVj6W73zins9/HiY6BWL99RVVBbJoiDFXZ\nw6dD9g0GKMo1+uGLyvJYXpnP/xxqo3vAT2l+VtLPefvGBfzxk5eOuyG1xWLMmCnMSf486ULCXQgx\nIXua+vj+U8djNwCNJb5yb3f5KMrJ5LpVFbxjfeIs6mh/vLnPgycQom8wQCgcoc8TpChnqJJ/x/pq\n9jb18eoZJ6V5tqSfM9NqYe0EV2F8x/pqblhTNaFzU5WsCimEmJDDLcYa6e3u5MsFxIuGe2e/jza3\nj/nFOfz4/RtGnFeal4Utw8LW3Wf5p+1HCYY1G+uK8YXCFOcOVdbv3VzLfz53EqcnOGq4n4sPXbbw\nDT/HbCeVuxBiQqLh3uacSLhH2zJ+2l3e2LZ1wymlqCmy83p7PxcuKOKmtfN45XQvDZ0DsWmKAHm2\nDD5iBvJkhPtcIOEuhJiQQ9FwH6dy9wXDDJg3CXW6fbS5xt4o+vL6Mq5eXs7P/m4j/3DdMgA8gXBs\namPUHZfUsaKqgItqi97IlzFnSFtGCBHzD/99gNJ8G1/asjzh+IA/RGO3sRZ6u2vkvPR4XeZdpACN\n3YP0+0JUjTH3/J6bVsXenl+cwwU1Dg42uygedsEzPzuTxz57+YS/lrlOKnchRMwzr3fy9NGOEceP\ntrnRGuYX22lzjV25R+8iLc3LorHL+IUwVuU+3PXm/qOFuek9m2WqSbgLIQBw+4wFuk51DxIMRxI+\ndqjZaMlcu6KSfl8o1nZJJrr+y8p5jtix0Xruydy4tgqHPZMVlam5YNdsIeEuhACgqdtYwCsY1gk7\nFoFxMbU838ba+UZgtyep3p8+2sGHf7Y7VrmvrCqIfWy8uefxaopy2P/1a9lg3nEqzo+EuxACSNxf\ndPguR4daXKypdsQq8LYkffeXT/bw9Oudse3qVs4bCvfygnOb4RK9A1WcPwl3IQQATfHhHrfLkScQ\n4mTXAKuqHcwzL4wm67v3m+vDPHO0k4LsjNgNSiW5WbHdjcT0kdkyQggATvd4qCzIJjNDJYT70TY3\nEQ1rqh2xCjxZW8btNfrw3mCYqsJcKsxzU3nxrVQmlbsQc0RznwdfcMQeOnS6fQz6Q5zuHqS2JIf6\n8vyEtkz0Yuqaage2DCuleVmxyt0TCMVaNPErO5bm2SjLN8L9XGbKiMkj4S7EHBCOaK6/9wV++GxD\nwvHdp3u56rt/5eO/3MvpHg91JbnUl+fR2DVIyJwxc7jVTWleVkIlHp3r/r2/HOeWH74MGOEebZWX\n5dmwZViZX2xnSYpuU5fqpC0jxBzQ4fbR7wuxx9w7FOC1VjcffPAVtIYXTnQDUFuaQ1mejUA4QlOv\nh8VleRxucbG62hG7yFlZYI9tjbe7qY82l49QOILbG2Ld/EL2nXFSmmfMUf/DJy8lzyYxMxOkchdi\nDojudnS4xRXbHelbj7+OLdPKY5+9nBLzhqGFJbksrzRmuRxtc+MLhjnROcCa6qE560vK8zjZNUC/\nL8jRNmMDDqc3SL8vyOp5Du64uJYt5o1IpXk2uZg6QyTchZgDWpxGpe32hTjb62X/WSfPHe/io5cv\noq40l49duQgwgntppbGf6KEWF0daXYQjmlVxNyRtWlRMMKx5eE8zgZDRuukbDOD2hXDYM/nGzau5\neHHJ9H+RIoH8vSTEHNDcOzQv/VCLi0debaYwJ5P3X1wLwIcvW8TGhSXUVxj98WWV+RxpceMwt6Hb\nUDe0WNeG2iIsCh588dTQ8zu9hCOa/GyJlNlCKnchZrn4TabPV3Ofl8KcTDKtiq27z/DM65189PJF\nsX641aJYF7fRxep5Dg61uNjV2Et9eV7CMrv52ZmsrnbQ4hz6hdFkLioWvyepmFkS7kLMYnubelnx\n9cfp7B9/DfWxNDuNmTBLK/J54UQ3DnsmHzCr9mRWVztweYO81NDN5kUjWyzRY9E9UJt6jbZPQbaE\n+2wh4S7ELHbgrAtfMMLpbs/4Jyfx8O6ztLm8tPR5qSmyxy6MfuSyheSPEcTR80IRzaZFI9d42bTQ\nOHbF0jIAzphr0RTYpS0zW0i4CzGLRbe06x30j3PmSJ1uH3c/cpB7nzpBi9NLTVEO166sYGVVAXdc\nWjfmY5dV5pNhMaY+blo4snLftKiE5ZX53HjBPLIzLbF1aaRynz3k16wQs1j0TtCewUDs2Df+fIRw\nRPPNm1eP+djoEgJ/3N9CMKypKbLzlhUVvGVFxbifNzvTytKKfPyhcOxO03h5tgwe/9wVABTlZHHW\nnGopPffZQ8JdiFkseido78BQuO9t6sMTGLmMwHDHzSUEfEFjumJ10cSX3QX451vWTOhiblHO0HIE\nMltm9pDvhBCzWLLKvd8Xomdg/DbNic4B8rMzCEc0nkCY+ecY7vGzZ8ZSlDtUrUu4zx7ScxdilopE\nNB3uZOEexO0L4QuG+fFzJ/n6nw4nfXxDxwDLK/N58/JygNhyvZOtyNzrNDvTgi1D7kadLSTchZil\negYDBMNGWyT+gqrbZyyt29Xv5/Ej7WzdfXbEao9aa4539rOkPJ8vXLuUf37HGnKypqaqjoa7XEyd\nXSTchZilokvpZloVPWbP3R8Kx27573D7aOnzEghF2H/WmfDYnsEATk+Q+vI8Fpfl8bebFkzZOIvM\ndWnkYursIuEuxCxyz7YjPHGkHRjqt9eX59NrtmX6fUMbU5/t89Bpbka9s9HY2s4fCvPhn+3mx8+d\nBGBpxdQvt1uUY4R6gfTbZxUJdyFmiC8Y5lT30NZ2oXCEX+xsYtv+VmBot6PV1QX0eQJorRmIC/f9\nZ4aq9V2NvQA09Xh4+vVOHnjBWPelviJvyr+OYrNyH+umKDH9JhTuSqktSqljSqkGpdSXk3x8gVLq\nWaXUPqXUQaXUDZM/VCHSyz3bjvC2H7wQa7O0u32EIzp2Q1Cby0emVbG0Ip9gWOP2hRIq91fNcF9d\nXcCrZ/rwh8Kxar8s39gJqTzJHPXJVpgjbZnZaNxwV0pZgfuA64GVwO1KqZXDTvvfwMNa6/XAbcAP\nJ3ugQqSTs70efre3GU8gHAvz6Jrrp7sH0VrT7vJS6cimxNz4oncwENuEGoitpf7OC2vwhyIcOOuK\nzYvfeudmHv30ZbENNqZSceyCqrRlZpOJVO4bgQatdaPWOgBsBW4edo4GCsy3HUDr5A1RiPRz37MN\nhCLGTJjozUbRcB8MhOkeCNDm8lFVYKc416i+ewf9sZkytgwLoYjGalFct6oSgNfb3bS5fCgF84ty\nqCiYnr1LC6M9d6ncZ5WJhHs1cDbu/WbzWLx7gPcppZqB7cCnJ2V0QqQhbyDMI682866LalAKTnQY\nywS09MUtodszyNleD1WF2bFdkroHhir3RWVGL73KkU2VIxt7ppXT3R7aXT5K82xkZUzf5bTo56uY\nhhaQmLjJ+gm4HfiZ1roGuAH4hVJqxHMrpe5USu1RSu3p6uqapE8tRGppcXoIhjWX15eyoDiHBnMN\nmOY+D1Zzsa6XGnpodflYN78wdsHSaMsYlfvislwAqgvtKKWoLcnhdM+gUe07pqdij7JnWdn+mcu4\nbePUTbcU524i4d4CzI97v8Y8Fu/DwMMAWusdQDZQOvyJtNb3a603aK03lJWVnd+IhUhx0UW2aors\n1JfnJ7RlVs8rwGpRPLzH+GN586KSpOEerdxrinIAWFiaa4a7l8ppasfEW1KeL3ulzjITCffdQL1S\naqFSKgvjgum2YeecAd4CoJRagRHuUpoLkURzLNxzqK/I41T3IMFwxNhQozSXmiI7LU5j56RlFUZo\n5tky6DHbMvZMK/PM6jy6WUZtSS5nez20Oqe/chez07jhrrUOAXcBTwBHMWbFHFFKfVMpdZN52heB\njyqlDgC/AT6oJ2NvMCHSUEuflyyrhbI8G/XleYQimsauQdqcPqoL7dSWGC2XjXXFWMw2TXFuFr2D\nfvp9IfKzM2IXS6MrPdaV5BAMawb8ISodU7OGjEgtE5q7pLXejnGhNP7Y1+Pefg24dHKHJkR6au7z\nMK8wG4tFxe4gfbGhm1BEU1OUw6A/xPMYG2JElefbaHP5KMnLIj87g1XVBSyryOdNdcaOSNFfCIBU\n7gKQJX+FmHbNfd5Yr3xxWR4WBf9t9thriuwEQsYiYJvjtrerLcnl5ZPdZGVYyM/OpDw/myc+f0Xs\n4wtLJdxFIll+QIhp1mzuZwrGTJNPXrWE19uNi6o1RXZuuaiGH9y+npVVBbHH1JXk0Oby0dXvT7pm\nenm+jexM43/nKmnLCCTchZhWvmCY7gE/1XFrq3/xrUv5xFWLqXJkU11kpyA7k5vWzku4u7TWrMwb\nOgeSLq1rsShqi41zygtkvrmQtowQ08ITCHHb/Tu5Zb1x/19N8VC4K6X40pbl3H3dslGXC6grMdo4\noYgedbejutIcugf8MiVRABLuQkyapp5BtIa6uP531MFmFwebXRyLtV9yRpwz1jow8RdMRwv3T19d\nH5tmKYS0ZYSYJHc+tJe7HzmY9GOHW1wA+M0VIGvOcT9Thz0zdjNTni35Gi6rqx1sWV15Ts8r0peE\nuxDAk691cM+2I+f9+IbOfo519NPc60n68UMtLioKbCwszSXDoijPP/cZLbVma0Y2oRYTIeEuBPDU\nax1s3X3mnB/X4vTSPeBn+yFj96SOfj/hyMj79w61uFhTXci33nkBX75+eWwNmXNRZ7ZmJNzFRMhP\niRBAvz+ILxghFI6QYZ14zfPJX+6lzeWLXcQMRzTdA/6E5XYH/CFOdQ9y89pqNi4sZuPC4tGebkxD\nlbssrSvGJ5W7EIDbayzINegPn9PjGrsG6ez3c6bXw+X1xlp5bS4ff9jXzHeeeJ1IRPNaqxutYU1N\nwTjPNrZo5S6bYoiJkJ8SIQC3uU76QCCEI2dilbHLG6TfH+LdF9UA8I711bxwops2p5efv9zE/rNO\nuvsDFOYaz7d6nuMNjfGKpWXcsr6aNTVv7HnE3CDhLgTg9prhHrdH6Xia+4yLp1cvL+f6NVX0DgYA\naHX5aOgcoDzfxm/NZQWqC+2Uv8GleItzs/jee9a9oecQc4eEuxAQ275uwJ8Y7qe7B3n0YCufevOS\nEfPQ45fuBSjKycSWYWH/WScD/hBf2rKMlfMcOD2B2PrrQkwXCXcx52mtY5X74LBwf/RgK9/9y3He\nddF8KoctyNUct+kGGDchVTmyefGEsZVBfUU+F9UWTfXwhUhKLqiKOc8XjMQ2qx5euUcr+hbnyPnr\nLX1ecrKssQ2iASod2fR5jF8U9eVSrYuZI+Eu5rzoxVRIEu5mRZ/stv7mPg81RfaEdk10RcaS3CxK\n8mQBLzFzJNzFnBcNcBjZlonuWZo83L0j1oiJtm6WSNUuZpiEu5jzEip33/C2zOiVe4vTm7B0Lwxt\nlFFfIeEuZpaEu5jzojcwgTHPPfFj0XBP7Lm7fUFc3uCIBcAqzemO0e3zhJgpEu5izouv3Ie3ZWIX\nVIdV7i3DpkFGrap2UJZvY9PCEoSYSTIVUsx50eo8J8s6si1jfqzF6UVrHbt4erJrAIDqYZV7daGd\n3V+9ZqqHLMS4pHIXc160Oq9yZDMQt7aM1hq3L0i+LQN/KELXgD92/GcvnaayIJsVVdJ+EbOThLuY\n89y+ILYMC8W5WQltGV8wQjCsWW4GePSi6o6TPexp6uOTb16MLUO2tBOzk4S7mPPc3hAF9kzybBkM\n+EM8f7yLz/92f6wXv7LKWM3xFzuauO77z/PxX+6losDGrRvmz+SwhRiThLuY89y+IPnZGeTaMhj0\nh3jiSDt/2NdCi9Oo1Jeb4f6HfS1oNFcuK+df33mBbEQtZjW5oCrmPLc3SEG2Ubn3+0Ox9stxczPr\nKkc28xzZVDiyeehDG2WzDJESJNzFnOf2hXCYbZlBfyg2p/1YhxHuBfZM/vzpy8jPziQrQ/7YFalB\nflLFnNHu8iW87w2E6R0M0O8LUmC2ZTyB8FDlHg337ExK8mwS7CKlyE+rmBOOtLrY/C9Ps7epN3bs\nnm1HuOHeF+gZCMQuqAL4QxEAjrUbc9kL7PIHrkg9Eu5iTth9ygj1k52DgDFX/bnjXbS7fbiiPfdh\ne5N2m/PaC6THLlLQhMJdKbVFKXVMKdWglPpyko9/Xym13/x3XCnlnPyhCnH+DrW4AehwG62Zph4P\n7W5frNVSYDfaMlE5WcZMmCyrBZu0Y0QKGvenVillBe4DrgdWArcrpVbGn6O1/rzWep3Weh3wH8Dv\np2KwQpyvwy0uADr7jWp816keAL72duNHuTTXRp5taGrj+gWFgBH6w7fXEyIVTKQk2Qg0aK0btdYB\nYCtw8xjn3w78ZjIGJ8Rk8AbCnOg0Lo529huV+87GXkrzsnjfpgU8/LGLuXHtPPJsRvvFYc+kvty4\nK1VaMiJVTSTcq4Gzce83m8dGUErVAguBZ0b5+J1KqT1KqT1dXV3nOlYhzsvRdjcRDVaLosPtR2vN\nrsYeNi0sQSnFxoXF2LOs5JqVe02RPbbpRr5dwl2kpsluJt4G/E5rHU72Qa31/VrrDVrrDWVlZZP8\nqYVILtqSeVNdEV39fpr7vLS6fGxaVJxwXnS2TE2RPbbpRkG2zJQRqWki4d4CxC+iUWMeS+Y2pCUj\nZoG9Tb0cbTMuoh5qdlGcm8Xa+YV09vt43bzzdNU8R8JjouFeXZgT23RD2jIiVU0k3HcD9UqphUqp\nLIwA3zb8JKXUcqAI2DG5QxTi3H3+twf4x21HADjc6mZ1tYOK/GyCYc3u08a0yOH7nDrsmSyryGfT\nouLYRtcyx12kqnF/crXWIaXUXcATgBV4UGt9RCn1TWCP1joa9LcBW7XWeuqGK8T4nJ4AZ3o99A4G\njIupHf1cvbyM8gIbAC81dFNZkI1jWD89w2rhic9fAYA/FEYpY+kBIVLRhMoSrfV2YPuwY18f9v49\nkzcsIc7fYXNO+4A/xONH2ghFNKvnOSjJM8L9tTY3ly0pHfM5bBlWfnDbetbNL5zy8QoxFeRvTpF2\nDpkXUAF+s8uY6LW62kHE/KOp6agpAAAVF0lEQVRS65EtmWRuXDtvagYoxDSQcBdp53Cri3mObHoG\nA7xyupfCnExqiuz4gpHYOdF57EKkK7mvWswqh5pdhCNDl21c3iCnugfP6TkOt7hYO78wtsnG6nkO\nlFLYs6zkmzNillaMX7kLkcok3MWsse9MHzf+vxd59GBr7Nj3nzzOzf/vRYLhyBiPHOLyBmnq8bC6\n2sGaajPcq4emPEYvqk6kLSNEKpNwF7PGowfbANh3Zmjdudda3bh9odiNSMN96/HXeWRvM8FwhH/8\n02Fuu38nAGuqHawxQ31NfLjnZ1OWb6MwJ2uqvgwhZgXpuYsZ0eL0Ul1oj72vteaxQ0a4R4Nca81x\nc02YXad6Wb+gKOE5TncP8p9/PQnAf718isMtbjYuLOZta6rYUFeEN1DAuy/q4/KlQzNj3n9xLT2D\ngSn92oSYDSTcxbTbcbKH2x/YyaOfvizWMtl/1kmry0d5vo0jrW7CEU3vYACnJwjAzsYePn7l4oTn\n2X7Y+GWweVExOxt7+drbV/LhyxbGPp6TlcF33r024TE3rKmayi9NiFlDwl1MqhMd/fxxfwtfvHYZ\nFkvypXKjd4geanFRW5LDP/7pCK+1ucm0Kj5+5WK++ehrnOoeiC3Pu7A0lz2n+3j+eBe/3nUGgPdt\nruWxQ+2snV/ILz68iaaeQZbIDBghYqTnLibV1t1nue/Zk7F2yqA/NOKcaNvlRMcALzV08/t9LQTC\nET5y+SIuNW8uOtTioqHT2ObuvZsWMOAPccd/vcLeM33sPdPHh362m0MtLt62ppJMq0WCXYhhJNzF\npIreQLSrsZc/7mthw/99ih5zu7qoWLh39nOoxUWGRbH9M5fzpS3LWVyWS3amhUPNbo539JOfncFN\na+dhtShWVBbw5Oev4MnPX0F9RR5KwfWrpc0iRDLSlhGTJhLRvNZq3Pq/s7GHfl8IbzDMgWYnVy+v\nAKBnwE+ry4dFGZW7Uor6inyyM4211DOsFlZUFbD/bB8ZVgv15XmUF2Sz7a5LWVCcQ765SuNvP3Yx\np7sHmV+cMzNfrBCznFTuY5A10EantR7x+pzuGWTAHyI3y8pLDd3saDS2sjvU7I495rAZ/pfVl9Hu\n9rGvqY/V8woSnueaFRW8esbJ7tO9sTtJV81zxIIdjOV54+evCyESzalw7+z3se6bf2FvU++45/5h\nXzMb//lpemXaXFL/9pfjvPX7zxOJu5s02pK59U3zcftChCOa3Cwrh1pc7DjZw4X/50l+8kIjAH+z\nzli3pd8fYk1NYkh/4srF3L5xPlrD0krppQtxPtIy3AOhCL/dfSbhNnaAI61unJ4gOxvHD/fHD7fT\n1e+PhZEY0un28cALjZzoHODVM32x44dbXGRlWHjf5loAFhTncO3KCg63uPjt7jP0eYK8cKKbupIc\nLqodmrM+vAK3WBT/9Ddr+I/b1/Oui2qm54sSIs2kZbg/e6yTLz1yKDblLqrJXKPkeEf/mI+PRDSv\nnDIe+/OXT9Mn1XuC+59vJBTRZFktbD/UHjt+qMXFisp8FpXmsrGumA9cXMvqagftbh+PH2nnhjWV\nbKgt4tqVFdQU5WDLsGBRsKKyYMTnsFgUN66dN2LNdSHExKTlBdWzvR4A2lzehOOne4zjJzoGxnz8\n8c5++jxBPnbFIu5/oZGfvniKL751KXf8127WzS/kC9cuTTj/7t8dAODb71qb7OmmzJ7Tvdz16304\nvQEuXFDErz6yCaWMueVaa7735HF++uKp2FK3VQ47D3zgoglNG3z+eBff+PMR/v096xPaJodbXPxy\nVxM3r5uH2xviscNt5Nqs/PTFU3gCYd67aQFKKR7++MWAcWEVwBeM8J43LeDKpUN75y4uyyMc0diz\nrJP2mgghDGkZ7s19Rqi3uXwJx0/3GJX7ya4BwhHNc8c72bSwhFxbBi+f7GZ5ZQHFuVnsMts279tc\nS3Ofl5+9fJoFJTk8f7yLnY09vHfTAirMPTYPt7h4eE8zAB+4uI7sTAsvn+w577GX5tm4fnVlLKT3\nn3VysNk54jxfMMy9T52gvCCbjQsr2XaglSdf62BVtYNnjnZwpNXN1t1nuWZFBYvLctHA719t4bb7\nd/GpNy/GOsoNRmBscnHvUyfwhyJ85y/H+K8PvolHD7bSPRDgB0+foCTXxj9ct4ydjT08dbSD/3im\ngWtWVLCkPI/b3jQ/4blWmRdLHfZMLllckvCxb9y86rxfJyHE2NI73J2J4d7U48FqUfhDER473MZd\nv97HBy+p46Z18/jbB3axuCyXrXdezM7GHqoL7cwvzuHTb1nC/xxq4yu/P0SVI5vOfj//+deT3HOT\nEUz3Pn2CgmzjZfzqHw9zsnOAgSQ37pyLOy6u5Z6bVhk37jz4Ci5vMOl5S8rz+NVHNlGSm8WBZiff\nfuIYbm8wdmfn+zfX8o2bVsXuFL11Qw3v/ckuvvHn18Ydw6p5BVxeX8aPnjvJHQ++wosN3QDML7bz\n649spsph5y0rKqgosHHdqkruuXFV0jtS87MzWb+gkLU1hWRaE7uAb6orPqfXRQgxcWkZ7i3OkZV7\nKBzhbK+HjXXF7Gjs4QdPnwDgN6+c4WCzk4LsDFqdPt7yb3/FGwzHduFZXlnAllWVPH6knS9cu5RX\nTvXyy51NPHW0AzB+kXzumnoA/v2pE9SV5PCnuy6l8Dx7xT967iQPvHAKi0VRmmfD5Q3ym49uTrr+\nuMOeSYYZmJ968xLu/t1BSnKz+NOnLqWuJBdHTuIYlpTn88LdV9PvS/7LIl5hTha+YJiH95zlxYZu\nPn/NUt63eQEF9sxYSBdkZ/Lyl98y5l8BAL//xCXIrFIhpldahntzn9Fbb3cP9dxbnT5CEc01KyvY\n0djD8Y4BllfmmzM+nNy9ZRmbFpaw9ZUzaODvLq2LPfarb1tBfUUef7O+msvry7BaFAFzffE3L8vg\nw5ctRCmF1nDbxvlUOeycr/91wwrCEXjwpVNYLYqrl5dz8bB2RjLvWF9Np9vHltWVY/bUszIssb1E\nx5Nry+D771lHh8vHrcPaLVHjBTuAUgo1/mlCiEmUduHu8gbp94VQCtrjKvdov31NtYMqRzZtLh8f\nvKSO/WedPHW0kw9cXEeeLSNhil7U/OIcvvjWZQBUOrL513dekPRzf37YhdbzoZTia29fgUbzix1N\nfOYt9RN6XKbVwl1XT+zccxF/AVQIkTrSLtyjVfvyygKOtrnxh8LYMqyxcK8ryWFJeR6d/X7euqqS\nd15Uw1duCJNnmz0vhVKKf7xxFZ+7ZqlMBRRCnJe0m+feYl5M3WBW4B0u4+Li6W4P9kwrZfk2/u7S\nOr60ZRnFuVlkWi2zNkBn67iEELNf2oV7dKbMhjoj3NtcXpr7PDx+uM1cSVBx9fIK7rxi8VhPI4QQ\nKW329CLeoOY+Dw8834jTGyQnyxqbX320zc3f/+4AA/4QP37/hhkepRBCTI+0CfdvP36MbQdaAVha\nkUelOWPl3qdP4PQGeeQTl4xYoEoIIdJVWrRlGjoH+PPBVt68rIzsTAu1Jbnk2TLIz86gzxPk7RfM\n48IFI2fBCCFEukqLyv2HzzaQnWHlu+9eS58nQE6W8WVVObIZ8A/wmauXzPAIhRBieqVFuD97rJO3\nX1BFSZ4t4QadLauruCoYpr5C1gQXQswtEwp3pdQW4F7ACvxEa/2vSc65FbgH0MABrfXfTuI4RzXo\nD9HnCbKwLHfEx4av3iiEEHPFuOGulLIC9wHXAs3AbqXUNq31a3Hn1ANfAS7VWvcppcqnasDDRdeR\nqSmSvTSFECJqIhdUNwINWutGrXUA2ArcPOycjwL3aa37ALTWnZM7zNFF70itKTr/9VyEECLdTCTc\nq4Gzce83m8fiLQWWKqVeUkrtNNs40yJ605KEuxBCDJmsC6oZQD1wFVADPK+UWqO1TthlQil1J3An\nwIIFCyblE7f0ecnKsFCaO7GVDoUQYi6YSOXeAsSv91pjHovXDGzTWge11qeA4xhhn0Brfb/WeoPW\nekNZ2eSsNtjc56Wm0J50owghhJirJhLuu4F6pdRCpVQWcBuwbdg5f8So2lFKlWK0aRoncZyjau7z\nUC0tGSGESDBuuGutQ8BdwBPAUeBhrfURpdQ3lVI3mac9AfQopV4DngX+QWt9/huJnoMWp1f67UII\nMcyEeu5a6+3A9mHHvh73tga+YP6bNt5AmO6BgEyDFEKIYVJ6bZkWp0yDFEKIZFI63GUapBBCJJfS\n4d5m7pH6RjakFkKIdJTS4e72BgEozJHt6IQQIl5Kh/uAP4RFgT3TOtNDEUKIWSWlw73fFyLPloFS\ncgOTEELES+lwH/CHyM+WlowQQgyX2uFuVu5CCCESpXa4+0PkZUu4CyHEcCkd7v1+qdyFECKZlA73\nAV9QKnchhEgitcPdHyJfKnchhBghtcNdLqgKIURSKRvu4YhmMBCWtowQQiSRsuE+4A8BSOUuhBBJ\npHy4F8hNTEIIMULqhrvPrNylLSOEECOkbrj7jRUhpS0jhBAjpWy490vlLoQQo0rZcI/23GWeuxBC\njJS64S6VuxBCjCp1w12mQgohxKhSNtyjPffcLAl3IYQYLmXDfcBcEdJikV2YhBBiuJQN935fUFoy\nQggxipQNd9moQwghRpey4d4vK0IKIcSoUjbcjc2xJdyFECKZ1A13n4S7EEKMJnXDXfZPFUKIUU0o\n3JVSW5RSx5RSDUqpLyf5+AeVUl1Kqf3mv49M/lATGT13We5XCCGSGbf0VUpZgfuAa4FmYLdSapvW\n+rVhp/5Wa33XFIxxhGA4woA/RGGOhLsQQiQzkcp9I9CgtW7UWgeArcDNUzussbm8xnK/Eu5CCJHc\nRMK9Gjgb936zeWy4dyqlDiqlfqeUmp/siZRSdyql9iil9nR1dZ3HcA1OTzTcs877OYQQIp1N1gXV\nPwN1WusLgCeBnyc7SWt9v9Z6g9Z6Q1lZ2Xl/MqcnAEChXSp3IYRIZiLh3gLEV+I15rEYrXWP1tpv\nvvsT4KLJGV5yQ5W7hLsQQiQzkXDfDdQrpRYqpbKA24Bt8Scopari3r0JODp5QxzJGe2526UtI4QQ\nyYw7W0ZrHVJK3QU8AViBB7XWR5RS3wT2aK23AZ9RSt0EhIBe4INTOOZYW8YhlbsQQiQ1obuAtNbb\nge3Djn097u2vAF+Z3KGNzuUNYlGyxZ4QQowmJe9QdXqCOOyZspa7EEKMIjXD3RuUaZBCCDGG1Ax3\nTwCHTIMUQohRpWS4u7xBmQYphBBjSMlw7/MEKJK2jBBCjColwz16QVUIIURyKRfuoXCEfp+sCCmE\nEGNJuXB3+0KArCsjhBBjSblwjy0aJj13IYQYVeqFu7mujCw9IIQQo0u5cHdFV4SUtowQQowq5cLd\n6ZW2jBBCjCf1wl0qdyGEGFfKhXt1oZ23rqygQMJdCCFGlXJr5r51VSVvXVU508MQQohZLeUqdyGE\nEOOTcBdCiDQk4S6EEGlIwl0IIdKQhLsQQqQhCXchhEhDEu5CCJGGJNyFECINKa31zHxipbqApvN8\neCnQPYnDmUyzdWwyrnMj4zp3s3Vs6TauWq112XgnzVi4vxFKqT1a6w0zPY5kZuvYZFznRsZ17mbr\n2ObquKQtI4QQaUjCXQgh0lCqhvv9Mz2AMczWscm4zo2M69zN1rHNyXGlZM9dCCHE2FK1chdCCDGG\nlAt3pdQWpdQxpVSDUurLMziO+UqpZ5VSrymljiilPmsev0cp1aKU2m/+u2EGxnZaKXXI/Px7zGPF\nSqknlVInzP8WTfOYlsW9JvuVUm6l1Odm6vVSSj2olOpUSh2OO5b0NVKGH5g/cweVUhdO87i+o5R6\n3fzcf1BKFZrH65RS3rjX7kfTPK5Rv3dKqa+Yr9cxpdR1UzWuMcb227hxnVZK7TePT8trNkY+TN/P\nmNY6Zf4BVuAksAjIAg4AK2doLFXAhebb+cBxYCVwD/D3M/w6nQZKhx37NvBl8+0vA9+a4e9jO1A7\nU68XcAVwIXB4vNcIuAF4DFDAZmDXNI/rrUCG+fa34sZVF3/eDLxeSb935v8HBwAbsND8f9Y6nWMb\n9vF/A74+na/ZGPkwbT9jqVa5bwQatNaNWusAsBW4eSYGorVu01q/ar7dDxwFqmdiLBN0M/Bz8+2f\nA38zg2N5C3BSa32+N7G9YVrr54HeYYdHe41uBh7Shp1AoVKqarrGpbX+i9Y6ZL67E6iZis99ruMa\nw83AVq21X2t9CmjA+H932semlFLArcBvpurzjzKm0fJh2n7GUi3cq4Gzce83MwsCVSlVB6wHdpmH\n7jL/tHpwutsfJg38RSm1Vyl1p3msQmvdZr7dDlTMwLiibiPxf7aZfr2iRnuNZtPP3YcwKryohUqp\nfUqp55RSl8/AeJJ972bT63U50KG1PhF3bFpfs2H5MG0/Y6kW7rOOUioPeAT4nNbaDfwnsBhYB7Rh\n/Ek43S7TWl8IXA98Sil1RfwHtfF34IxMk1JKZQE3Af9tHpoNr9cIM/kajUYp9VUgBPzKPNQGLNBa\nrwe+APxaKVUwjUOald+7YW4nsZCY1tcsST7ETPXPWKqFewswP+79GvPYjFBKZWJ8436ltf49gNa6\nQ2sd1lpHgAeYwj9HR6O1bjH/2wn8wRxDR/TPPPO/ndM9LtP1wKta6w5zjDP+esUZ7TWa8Z87pdQH\ngbcD7zVDAbPt0WO+vRejt710usY0xvduxl8vAKVUBnAL8Nvosel8zZLlA9P4M5Zq4b4bqFdKLTQr\nwNuAbTMxELOX91PgqNb6e3HH4/tk7wAOD3/sFI8rVymVH30b42LcYYzX6Q7ztDuAP03nuOIkVFIz\n/XoNM9prtA34gDmjYTPgivvTesoppbYAdwM3aa09ccfLlFJW8+1FQD3QOI3jGu17tw24TSllU0ot\nNMf1ynSNK841wOta6+bogel6zUbLB6bzZ2yqrxpP9j+Mq8rHMX7jfnUGx3EZxp9UB4H95r8bgF8A\nh8zj24CqaR7XIoyZCgeAI9HXCCgBngZOAE8BxTPwmuUCPYAj7tiMvF4Yv2DagCBGf/PDo71GGDMY\n7jN/5g4BG6Z5XA0Y/djoz9mPzHPfaX6P9wOvAjdO87hG/d4BXzVfr2PA9dP9vTSP/wz4+LBzp+U1\nGyMfpu1nTO5QFUKINJRqbRkhhBATIOEuhBBpSMJdCCHSkIS7EEKkIQl3IYRIQxLuQgiRhiTchRAi\nDUm4CyFEGvr/g17J7yDNwgsAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F9r-t1fvj6PW",
        "colab_type": "code",
        "outputId": "a50c7c5c-419b-4003-f3c3-6beef804dc9d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "model.predict(X1)[:10]\n",
        "# test = files.upload()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.9971458 ],\n",
              "       [0.15907758],\n",
              "       [0.9637408 ],\n",
              "       [0.9333396 ],\n",
              "       [0.9890728 ],\n",
              "       [0.91423446],\n",
              "       [0.9836037 ],\n",
              "       [0.99452394],\n",
              "       [0.9628898 ],\n",
              "       [0.956831  ]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yEvStJ2UHbKa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test=pd.read_csv('/content/test.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BjQBSPnfHja8",
        "colab_type": "code",
        "outputId": "c97bba36-e93f-4094-e79b-6495d1941946",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "np.shape(test)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(19750, 301)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ijkTB2ZZHmu9",
        "colab_type": "code",
        "outputId": "6e413362-bd52-4b57-be36-c8dca1866c22",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "test.columns\n",
        "\n"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['id', '0', '1', '2', '3', '4', '5', '6', '7', '8',\n",
              "       ...\n",
              "       '290', '291', '292', '293', '294', '295', '296', '297', '298', '299'],\n",
              "      dtype='object', length=301)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FnB2jYomHpnd",
        "colab_type": "code",
        "outputId": "35b2587c-c700-4729-d691-35e31840ab9f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "test2 = test.drop(columns='id')\n",
        "print(np.shape(test))\n",
        "# scaler = MinMaxScaler()\n"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(19750, 301)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1GKseW7ZiccC",
        "colab_type": "code",
        "outputId": "c8ff37ed-e237-4af1-9ecb-5d61fc65a154",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# scaler = MinMaxScaler()\n",
        "import copy\n",
        "test_x = test2[[i for i in np.array(col_var)]]\n",
        "test_x = StandardScaler().fit_transform(test_x)\n",
        "test_x2 = copy.deepcopy(test_x)\n",
        "test_x = np.array(test_x).reshape(-1,14,15,1)\n",
        "print(np.shape(test_x))\n",
        "test_x = np.array(test_x).reshape(-1,14,15,1)\n",
        "print(np.shape(test_x))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(19750, 14, 15, 1)\n",
            "(19750, 14, 15, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CbuBlxgxI-EJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pre = model.predict(test_x)\n",
        "y_pre = [0 if i<=.5 else 1 for i in y_pre]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QxSHX6eMR9ju",
        "colab_type": "code",
        "outputId": "bbcd9fc0-7610-42e8-c165-e7a99aff01d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "y_pre[:10]"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qSl8VGggJV-T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ypre = pd.DataFrame(y_pre)\n",
        "\n",
        "pd.concat([test[['id']], pd.DataFrame(y_pre,columns=['target'])],axis=1).to_csv('/content/submission.csv',index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1d56Zl3RWVMk",
        "colab_type": "text"
      },
      "source": [
        "### The results only yield 59.6% accuracy on leaderboard, I cannot even find my ranking on the leaderboard ~~~~ noooooooo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tYfkGFT3WppJ",
        "colab_type": "text"
      },
      "source": [
        "## model2: stacking model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eX6XH6C7WtWZ",
        "colab_type": "code",
        "outputId": "aefbdaa4-e02a-4e2d-d488-df95f1439a70",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from mlxtend.classifier import StackingClassifier\n",
        "from sklearn import model_selection\n",
        "from sklearn import svm\n",
        "import warnings\n",
        "warnings.simplefilter('ignore')"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/externals/six.py:31: DeprecationWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
            "  \"(https://pypi.org/project/six/).\", DeprecationWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQQSJ4-KXEDd",
        "colab_type": "code",
        "outputId": "e7f0beda-a360-4861-e6ee-8f1c83660487",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "clf1 = KNeighborsClassifier()\n",
        "clf2 = DecisionTreeClassifier()\n",
        "clf3 = XGBClassifier()\n",
        "clf4 = svm.SVC()\n",
        "mec = LogisticRegression()\n",
        "clf_f = StackingClassifier(classifiers=[clf1,clf2,clf3,clf4], meta_classifier = mec )\n",
        "print(np.shape(X_a),np.shape(y))\n",
        "for clf, label in zip([clf1,clf2,clf3,clf4, clf_f], ['KNN','DT','XGB','SVC','stakcing Classifier1']):\n",
        "                      scores = model_selection.cross_val_score(clf,X_a,y, cv=5, scoring = 'accuracy')\n",
        "                      print(np.round(scores.mean(),3),np.round(scores.std(),3),label)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(250, 210) (250,)\n",
            "0.636 0.056 KNN\n",
            "0.572 0.078 DT\n",
            "0.712 0.048 XGB\n",
            "0.696 0.029 SVC\n",
            "0.696 0.07 stakcing Classifier1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wbP84dR-joDN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GWBlGoRjZof_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = clf_f.fit(X_a,y)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WXSr78cVjveG",
        "colab_type": "code",
        "outputId": "94253e46-f8ed-476c-fff2-199a547120a5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(np.shape(test_x2))"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(19750, 210)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lAdqyyhvZskS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ypre2 = clf_f.predict(test_x2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5fDR94bDZ_Lc",
        "colab_type": "code",
        "outputId": "b4eb82f6-752f-4058-fcf5-78e57fcede23",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "ypre2[:10]"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u-m6hNnNaImy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "pd.concat([test[['id']], pd.DataFrame(ypre2,columns=['target'])],axis=1).to_csv('/content/submission2.csv',index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lx8-RSmqbiGW",
        "colab_type": "text"
      },
      "source": [
        "### Accuracy is 62.6%, why!!!!!!!!!! :("
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q6zr0Q9BbKry",
        "colab_type": "text"
      },
      "source": [
        "## the highest accuracy in leader board is around 93% on public board and 88% in private leaderboard, tomorrow check to see whether there are available notebooks to learn`~~~"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NCOEbQjKbg5A",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    }
  ]
}